{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14184/3411426957.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'reload_ext'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'autoreload'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'autoreload'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'2'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_prepper\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDataPrepper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataPrepper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "# LOAD CLASS AND DATASET \n",
    "# dataset: https://www.kaggle.com/rmisra/news-category-dataset\n",
    "# start:    each doc in corpus contains an article link, category and other (irrelevant) key/value pairs\n",
    "# end goal: each doc in corpus contains a sentence with POS tagging and gender polarity, and label vector with actual gender\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from classes.data_prepper import DataPrepper\n",
    "\n",
    "p = DataPrepper()\n",
    "\n",
    "data = p.load_json('../datasets/1_newsDataset.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILTER CATEGORIES\n",
    "\n",
    "men_categories = ['SPORTS', 'MONEY', 'BUSINESS']\n",
    "women_categories = ['WOMEN', 'STYLE & BEAUTY']\n",
    "\n",
    "data['articles'] = p.filter_articles(men_categories+women_categories, data['articles'])\n",
    "\n",
    "p.write_json('../datasets/2_filtered_news_data.json', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCRAPE LINKS\n",
    "\n",
    "scraped_data = {}\n",
    "scraped_data['articles'] = []\n",
    "textlessUrls = []\n",
    "\n",
    "for article in data['articles']:\n",
    "    text = p.scrape_url(article['link'], textlessUrls)\n",
    "    gender = 'M' if article['category'] in men_categories else 'W'\n",
    "    if text != \"\":\n",
    "        scraped_data['articles'].append({'gender': gender, 'text': text})\n",
    "\n",
    "p.write_json('../datasets/3_text_and_gender.json', scraped_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTANTIATE NLP FROM SPACY\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPLITTING TRAINING AND TEST DATA\n",
    "scraped_data = p.load_json('../datasets/3_text_and_gender.json')\n",
    "split = round(len(scraped_data['articles'])*0.75)\n",
    "scraped_data['articles'] = scraped_data['articles'][:split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CALCULATE WEIGHTS FOR EACH TERM\n",
    "# using tf-idf weighting from 'An Introduction to Information Retrieval (2009 Online Edition)'\n",
    "# written by Christopher D. Manning, Prabhakar Raghavan & Hinrich Sch√ºtze\n",
    "ignore_terms = ['.', ',', '...', ' ', '\\u2019', '  ', '(', ')', '?', '\\u00a3', '/', '\"', ':', ';', '-', '--', '\\u2015', \"'\", '!', '$', '#', '\\u2014', '   ', '[',']']\n",
    "ignore_ents = ['TIME', 'DATE', 'GPE', 'CARDINAL', 'PERSON', 'MONEY', 'PERCENT']\n",
    "\n",
    "m_weights, w_weights = p.get_weight s(scraped_data['articles'], nlp, ignore_terms, ignore_ents)\n",
    "\n",
    "p.write_json('../datasets/4_word_weight_m.json', m_weights)\n",
    "p.write_json('../datasets/4_word_weight_w.json', w_weights)\n",
    "\n",
    "p.write_json('../datasets/4_word_weight_m_ordered.json', p.order_dict(m_weights, 'desc'))\n",
    "p.write_json('../datasets/4_word_weight_w_ordered.json', p.order_dict(w_weights, 'desc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NORMALIZE WEIGHTS\n",
    "\n",
    "p.normalize_dict(m_weights)\n",
    "p.normalize_dict(w_weights)\n",
    "\n",
    "p.write_json('datasets/5_word_weight_m_norm.json', m_weights)\n",
    "p.write_json('datasets/5_word_weight_w_norm.json', w_weights)\n",
    "\n",
    "p.write_json('datasets/5_word_weight_m_norm_ordered.json', p.order_dict(m_weights, 'desc'))\n",
    "p.write_json('datasets/5_word_weight_w_norm_ordered.json', p.order_dict(w_weights, 'desc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CALCULATE POLARITY FOR EACH TERM\n",
    "# -1 (man) to 1 (woman)\n",
    "\n",
    "polarity_dict = p.get_polarity(w_weights, m_weights)\n",
    "p.write_json('../datasets/6_word_polarity.json', polarity_dict)\n",
    "p.write_json('../datasets/6_word_polarity_ordered.json', p.order_dict(polarity_dict, 'desc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRAP ALL INTO FINAL DATASET\n",
    "# split all texts into sentences into words, each assigned sentence #, word, lemma, pos, dep and polarity\n",
    "# runtime: ~31 min\n",
    "\n",
    "data = p.load_json('../datasets/3_text_and_gender.json')\n",
    "polarity_dict = p.load_json('../datasets/6_word_polarity.json')\n",
    "\n",
    "corpus =    {\n",
    "                'Sentence #': [], \n",
    "                'Word': [],\n",
    "                'Lemma': [],\n",
    "                'Tag': [],\n",
    "                'POS': [],\n",
    "                'Dep': [],\n",
    "                'Polarity': [],\n",
    "                'Gender': []\n",
    "            }\n",
    "\n",
    "sentenceCount = 1\n",
    "\n",
    "ignore_terms = ['.', ',', '...', ' ', '\\u2019', '  ', '(', ')', '?', '\\u00a3', '/', '\"', ':', ';', '-', '--', '\\u2015', \"'\", '!', '$', '#', '\\u2014', '   ', '[',']']\n",
    "ignore_ents = ['TIME', 'DATE', 'GPE', 'CARDINAL', 'PERSON', 'MONEY', 'PERCENT']\n",
    "\n",
    "for article in data['articles']:\n",
    "    doc = nlp(article['text'])\n",
    "    assert doc.has_annotation(\"SENT_START\")\n",
    "    for sent in doc.sents:\n",
    "        for token in sent:\n",
    "            if not token.is_stop and token.lemma_ not in ignore_terms and token.ent_type_ not in ignore_ents:\n",
    "                corpus['Sentence #'].append(sentenceCount)\n",
    "                corpus['Word'].append(token.text)\n",
    "                corpus['Lemma'].append(token.lemma_)\n",
    "                corpus['Tag'].append(token.tag_)\n",
    "                corpus['POS'].append(token.pos_)\n",
    "                corpus['Dep'].append(token.dep_)\n",
    "                corpus['Polarity'].append(polarity_dict[token.lemma_] if token.lemma_ in polarity_dict else 0)\n",
    "                corpus['Gender'].append(article['gender'])\n",
    "        sentenceCount += 1\n",
    "\n",
    "p.write_json('../datasets/7_dataset_w_tags.json', corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "(unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape (<ipython-input-5-037853430cd5>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-5-037853430cd5>\"\u001b[1;36m, line \u001b[1;32m4\u001b[0m\n\u001b[1;33m    df = pd.read_json('C:\\Users\\hanse\\OneDrive - Aarhus Universitet\\IKT-studie\\BachelorProjekt\\ML-Iterationer\\Sentiment\\dataset_backups\\11.10.2021/7_dataset_SM.json')\u001b[0m\n\u001b[1;37m                      ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m (unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape\n"
     ]
    }
   ],
   "source": [
    "# LOAD DATASET WITH PANDA \n",
    "import pandas as pd \n",
    "\n",
    "df = pd.read_json('../datasets/7_dataset_SM.json')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# SIMPLE DATASET\n",
    "# each row contains sentence and gender\n",
    "\n",
    "data = p.load_json('../datasets/3_text_and_gender_SM.json')\n",
    "polarity_dict = p.load_json('../datasets/6_word_polarity.json')\n",
    "\n",
    "corpus =    {\n",
    "                'Text': [],\n",
    "                'Gender': []\n",
    "            }\n",
    "\n",
    "ignore_terms = ['.', ',', '...', ' ', '\\u2019', '  ', '(', ')', '?', '\\u00a3', '/', '\"', ':', ';', '-', '--', '\\u2015', \"'\", '!', '$', '#', '\\u2014', '   ', '[',']']\n",
    "ignore_ents = ['TIME', 'DATE', 'GPE', 'CARDINAL', 'PERSON', 'MONEY', 'PERCENT']\n",
    "\n",
    "for article in data['articles']:\n",
    "    doc = nlp(article['text'])\n",
    "    assert doc.has_annotation(\"SENT_START\")\n",
    "    for sent in doc.sents:\n",
    "        text = p.preprocess_text(sent.text)\n",
    "        if text != \" \":\n",
    "            corpus['Text'].append(text)\n",
    "            corpus['Gender'].append(article['gender'])\n",
    "\n",
    "p.write_json('../datasets/8_dataset_simple_SM.json', corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'testing at my home from of yes'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = '.. . testing , \" at my home from of yes'\n",
    "\n",
    "p.preprocess_text(test)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5ce0bbaaba556c243eee45087b7684ce43fda2aa9a5b0798832d1be21bf73af7"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit (windows store)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

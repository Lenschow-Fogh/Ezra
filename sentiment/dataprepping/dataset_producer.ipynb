{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INITIALIZE CUSTOM HELPER CLASS AND SPACY\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from classes.data_prepper import DataPrepper\n",
    "\n",
    "p = DataPrepper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD CLASS AND DATASET \n",
    "# dataset: https://www.kaggle.com/rmisra/news-category-dataset\n",
    "# start:    each doc in corpus contains an article link, category and other (irrelevant) key/value pairs\n",
    "# end goal: each doc in corpus contains a sentece #, word, lemma, POS-tag, gender and polarity\n",
    "\n",
    "data = p.load_json('../datasets/1_newsDataset.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILTER CATEGORIES\n",
    "men_categories = ['SPORTS', 'MONEY', 'BUSINESS']\n",
    "women_categories = ['WOMEN', 'STYLE & BEAUTY']\n",
    "\n",
    "data['articles'] = p.filter_articles(men_categories+women_categories, data['articles'])\n",
    "\n",
    "p.write_json('../datasets/2_filtered_category.json', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCRAPE LINKS\n",
    "data = p.load_json('../datasets/2_filtered_news.json')\n",
    "\n",
    "scraped_data = {}\n",
    "scraped_data['articles'] = []\n",
    "textlessUrls = []\n",
    "\n",
    "for article in data['articles']:\n",
    "    text = p.scrape_url(article['link'], textlessUrls)\n",
    "    gender = 'M' if article['category'] in men_categories else 'F'\n",
    "    if text != \"\":\n",
    "        scraped_data['articles'].append({'gender': gender, 'text': text})\n",
    "\n",
    "p.write_json('../datasets/3_text_and_gender.json', scraped_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPLITTING TRAINING AND TEST DATA\n",
    "data = p.load_json('../datasets/3_text_and_gender.json')\n",
    "\n",
    "split = round(len(data['articles'])*0.75)\n",
    "\n",
    "training_data = data['articles'][:split]\n",
    "test_data = data['articles'][split:]\n",
    "\n",
    "p.write_json('../datasets/4_text_and_gender_training.json', training_data)\n",
    "p.write_json('../datasets/4_text_and_gender_test.json', test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CALCULATE WEIGHTS FOR EACH TERM\n",
    "# using tf-idf weighting from 'An Introduction to Information Retrieval (2009 Online Edition)'\n",
    "# written by Christopher D. Manning, Prabhakar Raghavan & Hinrich SchÃ¼tze\n",
    "import itertools\n",
    "\n",
    "data = p.load_json('../datasets/4_text_and_gender_training.json')\n",
    "\n",
    "ignore_terms = ['.', ',', '...', ' ', '\\u2019', '  ', '(', ')', '?', '\\u00a3', '/', '\"', ':', ';', '-', '--', '\\u2015', \"'\", '!', '$', '#', '\\u2014', '   ', '[',']']\n",
    "ignore_ents = ['TIME', 'DATE', 'GPE', 'CARDINAL', 'PERSON', 'MONEY', 'PERCENT']\n",
    "\n",
    "m_weights, w_weights = p.get_weights(data, nlp, ignore_terms, ignore_ents)\n",
    "\n",
    "m_weights = {k:v for k,v in m_weights.items() if v >= 0.05}\n",
    "w_weights = {k:v for k,v in w_weights.items() if v >= 0.05}\n",
    "\n",
    "m_weights = p.order_dict(m_weights, 'desc')\n",
    "w_weights = p.order_dict(w_weights, 'desc')\n",
    "\n",
    "print('len of m_weights is', len(m_weights), 'and w_weights is', len(w_weights), '\\nTo level the numbers to prevent bias, the two dicts are sliced. Dismissing the most neutral terms.')\n",
    "slice = 1500\n",
    "m_weights = dict(itertools.islice(m_weights.items(), slice))\n",
    "w_weights = dict(itertools.islice(w_weights.items(), slice))\n",
    "\n",
    "p.write_json('../datasets/5_word_weight_m.json', m_weights)\n",
    "p.write_json('../datasets/5_word_weight_f.json', w_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STANDARDINALIZE WEIGHTS\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np \n",
    "\n",
    "m_weights = p.load_json('../datasets/4_word_weight_m.json')\n",
    "s = StandardScaler()\n",
    "m_values = list(m_weights.values())\n",
    "m_values = np.array(m_values)\n",
    "m_values = m_values.reshape(-1,1)\n",
    "s.fit(m_values)\n",
    "m_values = s.transform(m_values)\n",
    "\n",
    "for i, (k, v) in enumerate(m_weights.items()):\n",
    "    m_weights[k] = float(m_values[i])\n",
    "\n",
    "w_weights = p.load_json('../datasets/4_word_weight_w.json')\n",
    "s = StandardScaler()\n",
    "w_values = list(w_weights.values())\n",
    "w_values = np.array(w_values)\n",
    "w_values = w_values.reshape(-1,1)\n",
    "s.fit(w_values)\n",
    "w_values = s.transform(w_values)\n",
    "\n",
    "for i, (k, v) in enumerate(w_weights.items()):\n",
    "    w_weights[k] = float(w_values[i])\n",
    "\n",
    "p.write_json('../datasets/5_word_weight_m_standard.json', m_weights)\n",
    "p.write_json('../datasets/5_word_weight_w_standard.json', w_weights)\n",
    "\n",
    "polarity_dict = p.get_polarity(w_weights, m_weights)\n",
    "polarity_dict = p.order_dict(polarity_dict, 'desc')\n",
    "\n",
    "p.write_json('../datasets/6_polarity_dict_standard.json', polarity_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NORMALIZE WEIGHTS\n",
    "m_weights = p.load_json('../datasets/5_word_weight_m.json')\n",
    "w_weights = p.load_json('../datasets/5_word_weight_f.json')\n",
    "\n",
    "p.normalize_dict(m_weights)\n",
    "p.normalize_dict(w_weights)\n",
    "\n",
    "m_weights = p.order_dict(m_weights, 'desc')\n",
    "w_weights = p.order_dict(w_weights, 'desc')\n",
    "\n",
    "p.write_json('../datasets/5_word_weight_m_norm.json', m_weights)\n",
    "p.write_json('../datasets/5_word_weight_f_norm.json', w_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CALCULATE POLARITY FOR EACH TERM\n",
    "# -1 (man) to 1 (woman)\n",
    "\n",
    "m_weights = p.load_json('../datasets/5_word_weight_m_norm.json')\n",
    "w_weights = p.load_json('../datasets/5_word_weight_f_norm.json')\n",
    "\n",
    "polarity_dict = p.get_polarity(w_weights, m_weights)\n",
    "polarity_dict = p.order_dict(polarity_dict, 'desc')\n",
    "\n",
    "p.write_json('../datasets/6_polarity_dict_norm.json', polarity_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALIZE NORMALIZED WEIGHTS AND POLARITY DISTRIBUTION\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "m_weights = p.load_json('../datasets/5_word_weight_m_norm.json')\n",
    "w_weights = p.load_json('../datasets/5_word_weight_f_norm.json')\n",
    "pols = p.load_json('../datasets/6_polarity_dict_norm.json')\n",
    "m_weights = list(m_weights.values())\n",
    "w_weights = list(w_weights.values())\n",
    "pols = list(pols.values())\n",
    "\n",
    "m_weights_rounded = []\n",
    "for weight in m_weights:\n",
    "    m_weights_rounded.append(round(float(-1*weight), 2))\n",
    "x, y = np.unique(m_weights_rounded, return_counts=True)\n",
    "\n",
    "\n",
    "plt.figure(figsize=[10,6])\n",
    "plt.plot(x, y, color='b', alpha=0.7)\n",
    "\n",
    "\n",
    "w_weights_rounded = []\n",
    "for weight in w_weights:\n",
    "    w_weights_rounded.append(round(float(weight), 2))\n",
    "x, y = np.unique(w_weights_rounded, return_counts=True)\n",
    "\n",
    "plt.plot(x, y, color='r', alpha=0.7)\n",
    "\n",
    "\n",
    "pols_rounded = []\n",
    "for weight in pols:\n",
    "    pols_rounded.append(round(float(weight), 2))\n",
    "x, y = np.unique(pols_rounded, return_counts=True)\n",
    "\n",
    "plt.plot(x, y, color='g', alpha=0.7)\n",
    "\n",
    "\n",
    "ax2 = plt.gca()\n",
    "\n",
    "plt.ylabel('Frequency of words', fontdict={'fontsize':13, 'fontweight': 'bold'})\n",
    "plt.xlabel('Polarity', fontdict={'fontsize':13, 'fontweight': 'bold'})\n",
    "plt.title(\"Distribution of normalized weights and polarities\", fontdict={'fontsize':14, 'fontweight': 'bold'})\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALIZE STANDARDALIZED WEIGHTS AND POLARITY DISTRIBUTION\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "m_weights = p.load_json('../datasets/5_word_weight_m_standard.json')\n",
    "w_weights = p.load_json('../datasets/5_word_weight_w_standard.json')\n",
    "pols = p.load_json('../datasets/6_polarity_dict_standard.json')\n",
    "m_weights = list(m_weights.values())\n",
    "w_weights = list(w_weights.values())\n",
    "pols = list(pols.values())\n",
    "# m_weights = np.array(m_weights)\n",
    "pol_to_rep = {\n",
    "  -1.0: 0,\n",
    "  -0.9: 1,\n",
    "  -0.8: 2,\n",
    "  -0.7: 3,\n",
    "  -0.6: 4,\n",
    "  -0.5: 5,\n",
    "  -0.4: 6,\n",
    "  -0.3: 7,\n",
    "  -0.2: 8,\n",
    "  -0.1: 9,\n",
    "  -0.0: 10,\n",
    "  0.0: 10,\n",
    "  0.1: 11,\n",
    "  0.2: 12,\n",
    "  0.3: 13,\n",
    "  0.4: 14,\n",
    "  0.5: 15,\n",
    "  0.6: 16,\n",
    "  0.7: 17,\n",
    "  0.8: 18,\n",
    "  0.9: 19,\n",
    "  1.0: 20,\n",
    "}\n",
    "\n",
    "m_weights_rounded = []\n",
    "for weight in m_weights:\n",
    "    m_weights_rounded.append(round(float(weight), 2))\n",
    "# m_weights_rounded = m_weights_rounded[:5000]\n",
    "x, y = np.unique(m_weights_rounded, return_counts=True)\n",
    "\n",
    "\n",
    "plt.figure(figsize=[10,6])\n",
    "# plt.bar(x, y, color='b', width=0.1, alpha=0.7, align='center')\n",
    "plt.plot(x, y, color='b', alpha=0.7)\n",
    "\n",
    "\n",
    "w_weights_rounded = []\n",
    "for weight in w_weights:\n",
    "    w_weights_rounded.append(round(float(weight), 2))\n",
    "# w_weights_rounded = w_weights_rounded[:5000]\n",
    "x, y = np.unique(w_weights_rounded, return_counts=True)\n",
    "\n",
    "# plt.bar(x, y, color='r', width=0.1, alpha=0.7, align='center')\n",
    "\n",
    "plt.plot(x, y, color='r', alpha=0.7)\n",
    "\n",
    "\n",
    "pols_rounded = []\n",
    "for weight in pols:\n",
    "    pols_rounded.append(round(float(weight), 2))\n",
    "# pols_rounded = pols_rounded[:5000]\n",
    "x, y = np.unique(pols_rounded, return_counts=True)\n",
    "\n",
    "# plt.bar(x, y, color='r', width=0.1, alpha=0.7, align='center')\n",
    "\n",
    "plt.plot(x, y, color='g', alpha=0.7)\n",
    "\n",
    "\n",
    "ax2 = plt.gca()\n",
    "\n",
    "plt.ylabel('Frequency of words', fontdict={'fontsize':13, 'fontweight': 'bold'})\n",
    "plt.xlabel('Polarity', fontdict={'fontsize':13, 'fontweight': 'bold'})\n",
    "plt.title(\"Distribution of standardized weights and polarities\", fontdict={'fontsize':14, 'fontweight': 'bold'})\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRAP ALL INTO FINAL DATASET\n",
    "# split all texts into sentences into words, each assigned sentence #, word, lemma, pos and polarity\n",
    "# runtime: ~102 min\n",
    "\n",
    "data = p.load_json('../datasets/3_text_and_gender.json')\n",
    "polarity_dict = p.load_json('../datasets/6_polarity_dict_norm.json')\n",
    "\n",
    "# Sentiment corpus\n",
    "corpus =    {\n",
    "                'Sentence #': [], \n",
    "                'Word': [],\n",
    "                'Lemma': [],\n",
    "                'POS': [],\n",
    "                'Polarity': [],\n",
    "                'Gender': []\n",
    "            }\n",
    "\n",
    "# a corpus just containing sentences numbers and text for simplicity\n",
    "sentences =    {\n",
    "                'Sentence #': [], \n",
    "                'Text': []\n",
    "            }\n",
    "\n",
    "sentenceCount = 1\n",
    "\n",
    "ignore_terms = ['.', ',', '...', ' ', '\\u2019', '  ', '(', ')', '?', '\\u00a3', '/', '\"', ':', ';', '-', '--', '\\u2015', \"'\", '!', '$', '#', '\\u2014', '   ', '[',']']\n",
    "ignore_ents = ['TIME', 'DATE', 'GPE', 'CARDINAL', 'PERSON', 'MONEY', 'PERCENT']\n",
    "\n",
    "for article in data['articles']:\n",
    "    doc = nlp(article['text'])\n",
    "    for sent in doc.sents:\n",
    "        preproccesed_sentence = p.preprocess_text(sent.text)\n",
    "        sentence = nlp(preproccesed_sentence)\n",
    "        for token in sentence:\n",
    "            if not token.is_stop and token.lemma_ not in ignore_terms and token.ent_type_ not in ignore_ents:\n",
    "                corpus['Sentence #'].append(sentenceCount)\n",
    "                corpus['Word'].append(token.text)\n",
    "                corpus['Lemma'].append(token.lemma_)\n",
    "                corpus['POS'].append(token.pos_)\n",
    "                corpus['Polarity'].append(polarity_dict[token.lemma_] if token.lemma_ in polarity_dict else 0)\n",
    "                corpus['Gender'].append(article['gender'])\n",
    "        \n",
    "        sentences['Sentence #'].append(sentenceCount)\n",
    "        sentences['Text'].append(sent.text)\n",
    "        sentenceCount += 1\n",
    "\n",
    "p.write_json('../datasets/7_sentences.json', sentences)\n",
    "p.write_json('../datasets/sentiment_corpus.json', corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FINAL WRAP-UP TEST ON ONLY THE FIRST TEXT\n",
    "# runtime: ~1 second\n",
    "\n",
    "data = p.load_json('../datasets/3_text_and_gender.json')\n",
    "polarity_dict = p.load_json('../datasets/6_polarity_dict_norm.json')\n",
    "\n",
    "article = data['articles'][0]\n",
    "\n",
    "# Sentiment corpus\n",
    "corpus =    {\n",
    "                'Sentence #': [], \n",
    "                'Word': [],\n",
    "                'Lemma': [],\n",
    "                'POS': [],\n",
    "                'Polarity': [],\n",
    "                'Gender': []\n",
    "            }\n",
    "\n",
    "# a corpus just containing sentences numbers and text for simplicity, and to see the preprocessed sentences\n",
    "sentences =    {\n",
    "                'Sentence #': [], \n",
    "                'Text': []\n",
    "            }\n",
    "\n",
    "sentenceCount = 1\n",
    "\n",
    "ignore_terms = ['.', ',', '...', ' ', '\\u2019', '  ', '(', ')', '?', '\\u00a3', '/', '\"', ':', ';', '-', '--', '\\u2015', \"'\", '!', '$', '#', '\\u2014', '   ', '[',']']\n",
    "ignore_ents = ['TIME', 'DATE', 'GPE', 'CARDINAL', 'PERSON', 'MONEY', 'PERCENT']\n",
    "\n",
    "\n",
    "doc = nlp(article['text'])\n",
    "for sent in doc.sents:\n",
    "        preproccesed_sentence = p.preprocess_text(sent.text)\n",
    "        sentence = nlp(preproccesed_sentence)\n",
    "        for token in sentence:\n",
    "            if not token.is_stop and token.lemma_ not in ignore_terms and token.ent_type_ not in ignore_ents:\n",
    "                corpus['Sentence #'].append(sentenceCount)\n",
    "                corpus['Word'].append(token.text)\n",
    "                corpus['Lemma'].append(token.lemma_)\n",
    "                corpus['POS'].append(token.pos_)\n",
    "                corpus['Polarity'].append(polarity_dict[token.lemma_] if token.lemma_ in polarity_dict else 0)\n",
    "                corpus['Gender'].append(article['gender'])\n",
    "        \n",
    "        sentences['Sentence #'].append(sentenceCount)\n",
    "        sentences['Text'].append(sent.text)\n",
    "        sentenceCount += 1\n",
    "\n",
    "print(pd.DataFrame(sentences).head(2))\n",
    "\n",
    "import pandas as pd\n",
    "pd.DataFrame(corpus).groupby(['Sentence #'],as_index=False)['Word', 'Lemma', 'POS', 'Polarity'].agg(lambda x: list(x))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5ce0bbaaba556c243eee45087b7684ce43fda2aa9a5b0798832d1be21bf73af7"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit (windows store)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

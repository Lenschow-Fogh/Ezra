{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constructor of DataPrepper\n"
     ]
    }
   ],
   "source": [
    "# LOAD CLASS AND DATASET \n",
    "# dataset: https://www.kaggle.com/rmisra/news-category-dataset\n",
    "# start:    each doc in corpus contains an article link, category and other (irrelevant) key/value pairs\n",
    "# end goal: each doc in corpus contains a sentence with POS tagging and gender polarity, and label vector with actual gender\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from classes.data_prepper import DataPrepper\n",
    "\n",
    "p = DataPrepper()\n",
    "\n",
    "data = p.load_json('datasets/1_newsDataset.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILTER CATEGORIES\n",
    "\n",
    "men_categories = ['SPORTS', 'MONEY', 'BUSINESS']\n",
    "women_categories = ['WOMEN', 'STYLE & BEAUTY']\n",
    "\n",
    "data['articles'] = p.filter_articles(men_categories+women_categories, data['articles'])\n",
    "\n",
    "p.write_json('datasets/2_filtered_news_data.json', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCRAPE LINKS\n",
    "\n",
    "scraped_data = {}\n",
    "scraped_data['articles'] = []\n",
    "textlessUrls = []\n",
    "\n",
    "for article in data['articles']:\n",
    "    text = p.scrape_url(article['link'], textlessUrls)\n",
    "    gender = 'M' if article['category'] in men_categories else 'W'\n",
    "    if text != \"\":\n",
    "        scraped_data['articles'].append({'gender': gender, 'text': text})\n",
    "\n",
    "p.write_json('datasets/3_text_and_gender.json', scraped_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTANTIATE NLP FROM SPACY\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CALCULATE WEIGHTS FOR EACH TERM\n",
    "# using tf-idf weighting from 'An Introduction to Information Retrieval (2009 Online Edition)'\n",
    "# written by Christopher D. Manning, Prabhakar Raghavan & Hinrich Sch√ºtze\n",
    "ignore_terms = ['.', ',', '...', ' ', '\\u2019', '  ', '(', ')', '?', '\\u00a3', '/', '\"', ':', ';', '-', '--', '\\u2015', \"'\", '!', '$', '#', '\\u2014', '   ', '[',']']\n",
    "ignore_ents = ['TIME', 'DATE', 'GPE', 'CARDINAL', 'PERSON', 'MONEY', 'PERCENT']\n",
    "\n",
    "m_weights, w_weights = p.get_weights(scraped_data['articles'], nlp, ignore_terms, ignore_ents)\n",
    "\n",
    "p.write_json('datasets/4_word_weight_m.json', m_weights)\n",
    "p.write_json('datasets/4_word_weight_w.json', w_weights)\n",
    "\n",
    "p.write_json('datasets/4_word_weight_m_ordered.json', p.order_dict(m_weights, 'desc'))\n",
    "p.write_json('datasets/4_word_weight_w_ordered.json', p.order_dict(w_weights, 'desc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NORMALIZE WEIGHTS\n",
    "\n",
    "p.normalize_dict(m_weights)\n",
    "p.normalize_dict(w_weights)\n",
    "\n",
    "p.write_json('datasets/5_word_weight_m_norm.json', m_weights)\n",
    "p.write_json('datasets/5_word_weight_w_norm.json', w_weights)\n",
    "\n",
    "p.write_json('datasets/5_word_weight_m_norm_ordered.json', p.order_dict(m_weights, 'desc'))\n",
    "p.write_json('datasets/5_word_weight_w_norm_ordered.json', p.order_dict(w_weights, 'desc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CALCULATE POLARITY FOR EACH TERM\n",
    "# -1 (man) to 1 (woman)\n",
    "\n",
    "polarity_dict = p.get_polarity(w_weights, m_weights)\n",
    "p.write_json('datasets/6_word_polarity.json', polarity_dict)\n",
    "p.write_json('datasets/6_word_polarity_ordered.json', p.order_dict(polarity_dict, 'desc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRAP ALL INTO FINAL DATASET\n",
    "# split all texts into sentences into words, each assigned sentence #, word, lemma, pos, dep and polarity\n",
    "# runtime: ~31 min\n",
    "\n",
    "data = p.load_json('datasets/3_text_and_gender.json')\n",
    "polarity_dict = p.load_json('datasets/6_word_polarity.json')\n",
    "\n",
    "corpus =    {\n",
    "                'Sentence #': [], \n",
    "                'Word': [],\n",
    "                'Lemma': [],\n",
    "                'POS': [],\n",
    "                'Dep': [],\n",
    "                'Polarity': [],\n",
    "                'Gender': []\n",
    "            }\n",
    "\n",
    "sentenceCount = 1\n",
    "\n",
    "ignore_terms = ['.', ',', '...', ' ', '\\u2019', '  ', '(', ')', '?', '\\u00a3', '/', '\"', ':', ';', '-', '--', '\\u2015', \"'\", '!', '$', '#', '\\u2014', '   ', '[',']']\n",
    "ignore_ents = ['TIME', 'DATE', 'GPE', 'CARDINAL', 'PERSON', 'MONEY', 'PERCENT']\n",
    "\n",
    "for article in data['articles']:\n",
    "    doc = nlp(article['text'])\n",
    "    assert doc.has_annotation(\"SENT_START\")\n",
    "    for sent in doc.sents:\n",
    "        for token in sent:\n",
    "            if not token.is_stop and token.lemma_ not in ignore_terms and token.ent_type_ not in ignore_ents:\n",
    "                corpus['Sentence #'].append(sentenceCount)\n",
    "                corpus['Word'].append(token.text)\n",
    "                corpus['Lemma'].append(token.lemma_)\n",
    "                corpus['POS'].append(token.pos_)\n",
    "                corpus['Dep'].append(token.dep_)\n",
    "                corpus['Polarity'].append(polarity_dict[token.lemma_] if token.lemma_ in polarity_dict else 0)\n",
    "                corpus['Gender'].append(article['gender'])\n",
    "        sentenceCount += 1\n",
    "\n",
    "p.write_json('datasets/7_dataset.json', corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>Lemma</th>\n",
       "      <th>POS</th>\n",
       "      <th>Dep</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>organizations</td>\n",
       "      <td>organization</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>-0.193345</td>\n",
       "      <td>W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>decided</td>\n",
       "      <td>decide</td>\n",
       "      <td>VERB</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>-0.034137</td>\n",
       "      <td>W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>drop</td>\n",
       "      <td>drop</td>\n",
       "      <td>VERB</td>\n",
       "      <td>xcomp</td>\n",
       "      <td>-0.066243</td>\n",
       "      <td>W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>women</td>\n",
       "      <td>woman</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>0.353161</td>\n",
       "      <td>W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>accused</td>\n",
       "      <td>accuse</td>\n",
       "      <td>VERB</td>\n",
       "      <td>advcl</td>\n",
       "      <td>-0.015817</td>\n",
       "      <td>W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>11</td>\n",
       "      <td>apologize</td>\n",
       "      <td>apologize</td>\n",
       "      <td>VERB</td>\n",
       "      <td>ccomp</td>\n",
       "      <td>-0.012648</td>\n",
       "      <td>W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>11</td>\n",
       "      <td>felt</td>\n",
       "      <td>feel</td>\n",
       "      <td>VERB</td>\n",
       "      <td>relcl</td>\n",
       "      <td>0.059949</td>\n",
       "      <td>W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>11</td>\n",
       "      <td>uncomfortable</td>\n",
       "      <td>uncomfortable</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>acomp</td>\n",
       "      <td>0.011855</td>\n",
       "      <td>W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>11</td>\n",
       "      <td>disrespected</td>\n",
       "      <td>disrespect</td>\n",
       "      <td>VERB</td>\n",
       "      <td>conj</td>\n",
       "      <td>-0.007225</td>\n",
       "      <td>W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>11</td>\n",
       "      <td>intent</td>\n",
       "      <td>intent</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>attr</td>\n",
       "      <td>-0.014406</td>\n",
       "      <td>W</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows √ó 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Sentence #           Word          Lemma   POS    Dep  Polarity Gender\n",
       "0            1  organizations   organization  NOUN  nsubj -0.193345      W\n",
       "1            1        decided         decide  VERB   ROOT -0.034137      W\n",
       "2            1           drop           drop  VERB  xcomp -0.066243      W\n",
       "3            1          women          woman  NOUN  nsubj  0.353161      W\n",
       "4            1        accused         accuse  VERB  advcl -0.015817      W\n",
       "..         ...            ...            ...   ...    ...       ...    ...\n",
       "95          11      apologize      apologize  VERB  ccomp -0.012648      W\n",
       "96          11           felt           feel  VERB  relcl  0.059949      W\n",
       "97          11  uncomfortable  uncomfortable   ADJ  acomp  0.011855      W\n",
       "98          11   disrespected     disrespect  VERB   conj -0.007225      W\n",
       "99          11         intent         intent  NOUN   attr -0.014406      W\n",
       "\n",
       "[100 rows x 7 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LOAD DATASET WITH PANDA \n",
    "import pandas as pd \n",
    "\n",
    "df = pd.read_json('datasets/7_dataset.json')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SIMPLE DATASET\n",
    "# each row contains sentence and gender\n",
    "\n",
    "data = p.load_json('datasets/3_text_and_gender.json')\n",
    "polarity_dict = p.load_json('datasets/6_word_polarity.json')\n",
    "\n",
    "corpus =    {\n",
    "                'Text': [],\n",
    "                'Gender': []\n",
    "            }\n",
    "\n",
    "ignore_terms = ['.', ',', '...', ' ', '\\u2019', '  ', '(', ')', '?', '\\u00a3', '/', '\"', ':', ';', '-', '--', '\\u2015', \"'\", '!', '$', '#', '\\u2014', '   ', '[',']']\n",
    "ignore_ents = ['TIME', 'DATE', 'GPE', 'CARDINAL', 'PERSON', 'MONEY', 'PERCENT']\n",
    "\n",
    "for article in data['articles']:\n",
    "    doc = nlp(article['text'])\n",
    "    assert doc.has_annotation(\"SENT_START\")\n",
    "    for sent in doc.sents:\n",
    "        corpus['Text'].append(p.preprocess_text(sent.text))\n",
    "        corpus['Gender'].append(article['gender'])\n",
    "\n",
    "p.write_json('datasets/8_dataset_simple.json', corpus)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f00922dfc658c9c6d5a8441e23ac7ecd5da2a81953be115dd7d5d471af81b74e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

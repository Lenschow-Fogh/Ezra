{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 130,
   "source": [
    "import json\r\n",
    "import spacy\r\n",
    "import math\r\n",
    "\r\n",
    "with open('../datasets/3_text_and_gender.json') as json_file:\r\n",
    "    data = json.load(json_file)\r\n",
    "\r\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "source": [
    "ignores = ['.', ',', '...', ' ', '\\u2019', '  ', '(', ')', '?', '\\u00a3', '/', '\"', ':', ';', '-', '--', '\\u2015', \"'\", '!', '$', '#', '\\u2014', '   ', '[',']']\r\n",
    "ent_types = ['TIME', 'DATE', 'GPE', 'CARDINAL', 'PERSON', 'MONEY', 'PERCENT']\r\n",
    "m_words = {}\r\n",
    "w_words = {}\r\n",
    "\r\n",
    "N = len(data['articles'])\r\n",
    "m_tf_ds = []\r\n",
    "m_df_t = {}\r\n",
    "w_tf_ds = []\r\n",
    "w_df_t = {}\r\n",
    "\r\n",
    "for article in data['articles']:\r\n",
    "    doc = nlp(article['text'])\r\n",
    "    tf_d = {}\r\n",
    "    if article['gender'] == 'M':\r\n",
    "        for token in doc:\r\n",
    "            if not token.is_stop and token.lemma_ not in ignores and token.ent_type_ not in ent_types:\r\n",
    "                word = token.lemma_\r\n",
    "                if word in tf_d:\r\n",
    "                    tf_d[word] += 1\r\n",
    "                else:\r\n",
    "                    tf_d[word] = 1\r\n",
    "        m_tf_ds.append(tf_d)\r\n",
    "        for key in tf_d:\r\n",
    "            if key in m_df_t:\r\n",
    "                m_df_t[key] += 1\r\n",
    "            else:\r\n",
    "                m_df_t[key] = 1\r\n",
    "    if article['gender'] == 'W':\r\n",
    "        for token in doc:\r\n",
    "            if not token.is_stop and token.lemma_ not in ignores and token.ent_type_ not in ent_types:\r\n",
    "                word = token.lemma_\r\n",
    "                if word in tf_d:\r\n",
    "                    tf_d[word] += 1\r\n",
    "                else:\r\n",
    "                    tf_d[word] = 1\r\n",
    "        w_tf_ds.append(tf_d)\r\n",
    "        for key in tf_d:\r\n",
    "            if key in w_df_t:\r\n",
    "                w_df_t[key] += 1\r\n",
    "            else:\r\n",
    "                w_df_t[key] = 1\r\n",
    "\r\n",
    "m_tf_idfs = {}\r\n",
    "for tf_dict in m_tf_ds:\r\n",
    "    tf_idf = {}\r\n",
    "    for k, v in tf_dict.items():\r\n",
    "        tf_idf = v*(math.log(N/m_df_t[k]))\r\n",
    "        if k in m_tf_idfs:\r\n",
    "            m_tf_idfs[k].append(tf_idf)\r\n",
    "        else:\r\n",
    "            m_tf_idfs[k] = [tf_idf]\r\n",
    "\r\n",
    "w_tf_idfs = {}\r\n",
    "for tf_dict in w_tf_ds:\r\n",
    "    tf_idf = {}\r\n",
    "    for k, v in tf_dict.items():\r\n",
    "        tf_idf = v*(math.log(N/w_df_t[k]))\r\n",
    "        if k in w_tf_idfs:\r\n",
    "            w_tf_idfs[k].append(tf_idf)\r\n",
    "        else:\r\n",
    "            w_tf_idfs[k] = [tf_idf]\r\n",
    "\r\n",
    "for k, v in m_tf_idfs.items():\r\n",
    "    # length = len(v)\r\n",
    "    m_tf_idfs[k] = sum(v)/N\r\n",
    "\r\n",
    "for k, v in w_tf_idfs.items():\r\n",
    "    # length = len(v)\r\n",
    "    w_tf_idfs[k] = sum(v)/N\r\n",
    "\r\n",
    "\r\n",
    "with open('../datasets/4_word_weight_m.json', \"w\") as outFile:\r\n",
    "    json.dump(m_tf_idfs, outFile)\r\n",
    "\r\n",
    "with open('../datasets/4_word_weight_w.json', \"w\") as outFile:\r\n",
    "    json.dump(w_tf_idfs, outFile)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "source": [
    "from collections import OrderedDict\r\n",
    "\r\n",
    "m_descending = OrderedDict(sorted(m_tf_idfs.items(), key=lambda kv: kv[1], reverse=True))\r\n",
    "w_descending = OrderedDict(sorted(w_tf_idfs.items(), key=lambda kv: kv[1], reverse=True))\r\n",
    "\r\n",
    "with open('../datasets/4_word_weight_m_ordered.json', \"w\") as outFile:\r\n",
    "    json.dump(m_descending, outFile)\r\n",
    "\r\n",
    "with open('../datasets/4_word_weight_w_ordered.json', \"w\") as outFile:\r\n",
    "    json.dump(w_descending, outFile)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.7",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit (windows store)"
  },
  "interpreter": {
   "hash": "5ce0bbaaba556c243eee45087b7684ce43fda2aa9a5b0798832d1be21bf73af7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
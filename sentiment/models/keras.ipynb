{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 16115166817913871350\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from itertools import chain\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "from itertools import chain\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_json('../datasets/7_dataset_SM.json')\n",
    "df = df[:5000]\n",
    "df = df.fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>Lemma</th>\n",
       "      <th>POS</th>\n",
       "      <th>Dep</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>At</td>\n",
       "      <td>at</td>\n",
       "      <td>ADV</td>\n",
       "      <td>advmod</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>least</td>\n",
       "      <td>least</td>\n",
       "      <td>ADV</td>\n",
       "      <td>advmod</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>two</td>\n",
       "      <td>two</td>\n",
       "      <td>NUM</td>\n",
       "      <td>nummod</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>organizations</td>\n",
       "      <td>organization</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>-0.193345</td>\n",
       "      <td>W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>have</td>\n",
       "      <td>have</td>\n",
       "      <td>AUX</td>\n",
       "      <td>aux</td>\n",
       "      <td>-0.046438</td>\n",
       "      <td>W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>235</td>\n",
       "      <td>not</td>\n",
       "      <td>not</td>\n",
       "      <td>PART</td>\n",
       "      <td>neg</td>\n",
       "      <td>-0.002688</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>235</td>\n",
       "      <td>signed</td>\n",
       "      <td>sign</td>\n",
       "      <td>VERB</td>\n",
       "      <td>relcl</td>\n",
       "      <td>-0.096119</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>235</td>\n",
       "      <td>by</td>\n",
       "      <td>by</td>\n",
       "      <td>ADP</td>\n",
       "      <td>agent</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>235</td>\n",
       "      <td>an</td>\n",
       "      <td>an</td>\n",
       "      <td>DET</td>\n",
       "      <td>det</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>235</td>\n",
       "      <td>NFL</td>\n",
       "      <td>NFL</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>compound</td>\n",
       "      <td>-0.310746</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Sentence #           Word         Lemma    POS       Dep  Polarity Label\n",
       "0              1             At            at    ADV    advmod  0.000000     W\n",
       "1              1          least         least    ADV    advmod  0.000000     W\n",
       "2              1            two           two    NUM    nummod  0.000000     W\n",
       "3              1  organizations  organization   NOUN     nsubj -0.193345     W\n",
       "4              1           have          have    AUX       aux -0.046438     W\n",
       "...          ...            ...           ...    ...       ...       ...   ...\n",
       "4995         235            not           not   PART       neg -0.002688     M\n",
       "4996         235         signed          sign   VERB     relcl -0.096119     M\n",
       "4997         235             by            by    ADP     agent  0.000000     M\n",
       "4998         235             an            an    DET       det  0.000000     M\n",
       "4999         235            NFL           NFL  PROPN  compound -0.310746     M\n",
       "\n",
       "[5000 rows x 7 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create mapper for features\n",
    "\n",
    "lemma_vocab = np.unique(df['Lemma']).tolist()\n",
    "POS_vocab = np.unique(df['POS']).tolist()\n",
    "# dep_vocab = np.unique(df['Dep']).tolist()\n",
    "\n",
    "i_to_lemma = {i:lemma for  i, lemma in enumerate(lemma_vocab)}\n",
    "lemma_to_i = {lemma:i for  i, lemma in enumerate(lemma_vocab)}\n",
    "\n",
    "i_to_POS = {i:POS for  i, POS in enumerate(POS_vocab)}\n",
    "POS_to_i = {POS:i for  i, POS in enumerate(POS_vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hanse\\AppData\\Local\\Temp/ipykernel_5180/1779994312.py:6: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
      "  df_grouped = df.groupby(['Sentence #'],as_index=False)['Word', 'Lemma', 'POS', 'Dep', 'Polarity', 'Lemma_index', 'POS_index', 'Label'].agg(lambda x: list(x))\n"
     ]
    }
   ],
   "source": [
    "#Append index columns for lemmas and POS and group dataframe by sentence\n",
    "\n",
    "df['Lemma_index'] = df['Lemma'].map(lemma_to_i)\n",
    "df['POS_index'] = df['POS'].map(POS_to_i)\n",
    "\n",
    "df_grouped = df.groupby(['Sentence #'],as_index=False)['Word', 'Lemma', 'POS', 'Dep', 'Polarity', 'Lemma_index', 'POS_index', 'Label'].agg(lambda x: list(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>Lemma</th>\n",
       "      <th>POS</th>\n",
       "      <th>Dep</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Lemma_index</th>\n",
       "      <th>POS_index</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[At, least, two, organizations, have, decided,...</td>\n",
       "      <td>[at, least, two, organization, have, decide, t...</td>\n",
       "      <td>[ADV, ADV, NUM, NOUN, AUX, VERB, PART, VERB, P...</td>\n",
       "      <td>[advmod, advmod, nummod, nsubj, aux, ROOT, aux...</td>\n",
       "      <td>[0.0, 0.0, 0.0, -0.19334491413051003, -0.04643...</td>\n",
       "      <td>[373, 789, 1239, 915, 681, 528, 1214, 561, 200...</td>\n",
       "      <td>[2, 2, 8, 7, 3, 16, 9, 16, 11, 11, 1, 8, 7, 16...</td>\n",
       "      <td>[W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[Women, who, previously, worked, with, the, Os...</td>\n",
       "      <td>[woman, who, previously, work, with, the, Osca...</td>\n",
       "      <td>[NOUN, PRON, ADV, VERB, ADP, DET, PROPN, PUNCT...</td>\n",
       "      <td>[nsubj, nsubj, advmod, relcl, prep, det, npadv...</td>\n",
       "      <td>[0.35316112487835105, 0.0, -0.0393333114017160...</td>\n",
       "      <td>[1307, 1298, 990, 1308, 1304, 1194, 221, 11, 1...</td>\n",
       "      <td>[7, 10, 2, 16, 1, 5, 11, 12, 16, 7, 16, 11, 13...</td>\n",
       "      <td>[W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>[In, response, to, the, allegations, ,, Visa, ...</td>\n",
       "      <td>[in, response, to, the, allegation, ,, Visa, a...</td>\n",
       "      <td>[ADP, NOUN, ADP, DET, NOUN, PUNCT, PROPN, VERB...</td>\n",
       "      <td>[prep, pobj, prep, det, pobj, punct, nsubj, RO...</td>\n",
       "      <td>[-0.005527309776069, -0.039620517765398, -0.0,...</td>\n",
       "      <td>[719, 1064, 1214, 1194, 332, 10, 278, 347, 755...</td>\n",
       "      <td>[1, 7, 1, 5, 7, 12, 11, 16, 10, 3, 16, 10, 1, ...</td>\n",
       "      <td>[W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>[ , We, are, aware, of, the, allegations, that...</td>\n",
       "      <td>[ , we, be, aware, of, the, allegation, that, ...</td>\n",
       "      <td>[SPACE, PRON, AUX, ADJ, ADP, DET, NOUN, DET, A...</td>\n",
       "      <td>[punct, nsubj, ROOT, acomp, prep, det, pobj, n...</td>\n",
       "      <td>[0.0, -0.0, 0.0, -0.035281092794016006, 0.0, 0...</td>\n",
       "      <td>[0, 1284, 393, 385, 899, 1194, 332, 1193, 681,...</td>\n",
       "      <td>[14, 10, 3, 0, 1, 5, 7, 5, 3, 3, 16, 1, 11, 11...</td>\n",
       "      <td>[W, W, W, W, W, W, W, W, W, W, W, W, W, W, W]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>[At, this, point, ,, Visa, will, be, suspendin...</td>\n",
       "      <td>[at, this, point, ,, Visa, will, be, suspend, ...</td>\n",
       "      <td>[ADP, DET, NOUN, PUNCT, PROPN, AUX, AUX, VERB,...</td>\n",
       "      <td>[prep, det, pobj, punct, nsubj, aux, aux, ccom...</td>\n",
       "      <td>[0.0, 0.0, -0.19485058785637502, 0.0, -0.02323...</td>\n",
       "      <td>[373, 1204, 962, 10, 278, 1299, 393, 1179, 918...</td>\n",
       "      <td>[1, 5, 7, 12, 11, 3, 3, 16, 10, 7, 1, 5, 5, 7,...</td>\n",
       "      <td>[W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>231</td>\n",
       "      <td>[We, will, continue, the, conversation, this, ...</td>\n",
       "      <td>[we, will, continue, the, conversation, this, ...</td>\n",
       "      <td>[PRON, AUX, VERB, DET, NOUN, DET, NOUN, PUNCT]</td>\n",
       "      <td>[nsubj, aux, ROOT, det, dobj, det, npadvmod, p...</td>\n",
       "      <td>[-0.0, -0.005636411889281, -0.1172245756221810...</td>\n",
       "      <td>[1284, 1299, 496, 1194, 499, 1204, 855, 12]</td>\n",
       "      <td>[10, 3, 16, 5, 7, 5, 7, 12]</td>\n",
       "      <td>[M, M, M, M, M, M, M, M]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>232</td>\n",
       "      <td>[ ]</td>\n",
       "      <td>[ ]</td>\n",
       "      <td>[SPACE]</td>\n",
       "      <td>[ROOT]</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[14]</td>\n",
       "      <td>[M]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>233</td>\n",
       "      <td>[The, league, also, debated, the, issue, in, p...</td>\n",
       "      <td>[the, league, also, debate, the, issue, in, pr...</td>\n",
       "      <td>[DET, NOUN, ADV, VERB, DET, NOUN, ADP, ADJ, NO...</td>\n",
       "      <td>[det, nsubj, advmod, ROOT, det, dobj, prep, am...</td>\n",
       "      <td>[0.0, -0.18007909868621702, 0.0, -0.0284715738...</td>\n",
       "      <td>[1194, 787, 338, 526, 1194, 754, 719, 989, 839...</td>\n",
       "      <td>[5, 7, 2, 16, 5, 7, 1, 0, 7, 13, 0, 11, 7, 11,...</td>\n",
       "      <td>[M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>234</td>\n",
       "      <td>[The, movement, ,, which, grew, to, include, o...</td>\n",
       "      <td>[the, movement, ,, which, grow, to, include, o...</td>\n",
       "      <td>[DET, NOUN, PUNCT, DET, VERB, PART, VERB, ADJ,...</td>\n",
       "      <td>[det, nsubj, punct, nsubj, relcl, aux, advcl, ...</td>\n",
       "      <td>[0.0, 0.011468645017082, 0.0, 0.0, -0.12359977...</td>\n",
       "      <td>[1194, 861, 10, 1295, 671, 1214, 723, 917, 957...</td>\n",
       "      <td>[5, 7, 12, 5, 16, 9, 16, 0, 7, 4, 7, 12, 16, 1...</td>\n",
       "      <td>[M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>235</td>\n",
       "      <td>[Kaepernick, ,, who, was, not, signed, by, an,...</td>\n",
       "      <td>[Kaepernick, ,, who, be, not, sign, by, an, NFL]</td>\n",
       "      <td>[PROPN, PUNCT, PRON, AUX, PART, VERB, ADP, DET...</td>\n",
       "      <td>[nsubj, punct, nsubjpass, auxpass, neg, relcl,...</td>\n",
       "      <td>[-0.000288434531965, 0.0, 0.0, 0.0, -0.0026880...</td>\n",
       "      <td>[163, 10, 1298, 393, 888, 1118, 436, 343, 207]</td>\n",
       "      <td>[11, 12, 10, 3, 9, 16, 1, 5, 11]</td>\n",
       "      <td>[M, M, M, M, M, M, M, M, M]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>235 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Sentence #                                               Word  \\\n",
       "0             1  [At, least, two, organizations, have, decided,...   \n",
       "1             2  [Women, who, previously, worked, with, the, Os...   \n",
       "2             3  [In, response, to, the, allegations, ,, Visa, ...   \n",
       "3             4  [ , We, are, aware, of, the, allegations, that...   \n",
       "4             5  [At, this, point, ,, Visa, will, be, suspendin...   \n",
       "..          ...                                                ...   \n",
       "230         231  [We, will, continue, the, conversation, this, ...   \n",
       "231         232                                                [ ]   \n",
       "232         233  [The, league, also, debated, the, issue, in, p...   \n",
       "233         234  [The, movement, ,, which, grew, to, include, o...   \n",
       "234         235  [Kaepernick, ,, who, was, not, signed, by, an,...   \n",
       "\n",
       "                                                 Lemma  \\\n",
       "0    [at, least, two, organization, have, decide, t...   \n",
       "1    [woman, who, previously, work, with, the, Osca...   \n",
       "2    [in, response, to, the, allegation, ,, Visa, a...   \n",
       "3    [ , we, be, aware, of, the, allegation, that, ...   \n",
       "4    [at, this, point, ,, Visa, will, be, suspend, ...   \n",
       "..                                                 ...   \n",
       "230  [we, will, continue, the, conversation, this, ...   \n",
       "231                                                [ ]   \n",
       "232  [the, league, also, debate, the, issue, in, pr...   \n",
       "233  [the, movement, ,, which, grow, to, include, o...   \n",
       "234   [Kaepernick, ,, who, be, not, sign, by, an, NFL]   \n",
       "\n",
       "                                                   POS  \\\n",
       "0    [ADV, ADV, NUM, NOUN, AUX, VERB, PART, VERB, P...   \n",
       "1    [NOUN, PRON, ADV, VERB, ADP, DET, PROPN, PUNCT...   \n",
       "2    [ADP, NOUN, ADP, DET, NOUN, PUNCT, PROPN, VERB...   \n",
       "3    [SPACE, PRON, AUX, ADJ, ADP, DET, NOUN, DET, A...   \n",
       "4    [ADP, DET, NOUN, PUNCT, PROPN, AUX, AUX, VERB,...   \n",
       "..                                                 ...   \n",
       "230     [PRON, AUX, VERB, DET, NOUN, DET, NOUN, PUNCT]   \n",
       "231                                            [SPACE]   \n",
       "232  [DET, NOUN, ADV, VERB, DET, NOUN, ADP, ADJ, NO...   \n",
       "233  [DET, NOUN, PUNCT, DET, VERB, PART, VERB, ADJ,...   \n",
       "234  [PROPN, PUNCT, PRON, AUX, PART, VERB, ADP, DET...   \n",
       "\n",
       "                                                   Dep  \\\n",
       "0    [advmod, advmod, nummod, nsubj, aux, ROOT, aux...   \n",
       "1    [nsubj, nsubj, advmod, relcl, prep, det, npadv...   \n",
       "2    [prep, pobj, prep, det, pobj, punct, nsubj, RO...   \n",
       "3    [punct, nsubj, ROOT, acomp, prep, det, pobj, n...   \n",
       "4    [prep, det, pobj, punct, nsubj, aux, aux, ccom...   \n",
       "..                                                 ...   \n",
       "230  [nsubj, aux, ROOT, det, dobj, det, npadvmod, p...   \n",
       "231                                             [ROOT]   \n",
       "232  [det, nsubj, advmod, ROOT, det, dobj, prep, am...   \n",
       "233  [det, nsubj, punct, nsubj, relcl, aux, advcl, ...   \n",
       "234  [nsubj, punct, nsubjpass, auxpass, neg, relcl,...   \n",
       "\n",
       "                                              Polarity  \\\n",
       "0    [0.0, 0.0, 0.0, -0.19334491413051003, -0.04643...   \n",
       "1    [0.35316112487835105, 0.0, -0.0393333114017160...   \n",
       "2    [-0.005527309776069, -0.039620517765398, -0.0,...   \n",
       "3    [0.0, -0.0, 0.0, -0.035281092794016006, 0.0, 0...   \n",
       "4    [0.0, 0.0, -0.19485058785637502, 0.0, -0.02323...   \n",
       "..                                                 ...   \n",
       "230  [-0.0, -0.005636411889281, -0.1172245756221810...   \n",
       "231                                              [0.0]   \n",
       "232  [0.0, -0.18007909868621702, 0.0, -0.0284715738...   \n",
       "233  [0.0, 0.011468645017082, 0.0, 0.0, -0.12359977...   \n",
       "234  [-0.000288434531965, 0.0, 0.0, 0.0, -0.0026880...   \n",
       "\n",
       "                                           Lemma_index  \\\n",
       "0    [373, 789, 1239, 915, 681, 528, 1214, 561, 200...   \n",
       "1    [1307, 1298, 990, 1308, 1304, 1194, 221, 11, 1...   \n",
       "2    [719, 1064, 1214, 1194, 332, 10, 278, 347, 755...   \n",
       "3    [0, 1284, 393, 385, 899, 1194, 332, 1193, 681,...   \n",
       "4    [373, 1204, 962, 10, 278, 1299, 393, 1179, 918...   \n",
       "..                                                 ...   \n",
       "230        [1284, 1299, 496, 1194, 499, 1204, 855, 12]   \n",
       "231                                                [0]   \n",
       "232  [1194, 787, 338, 526, 1194, 754, 719, 989, 839...   \n",
       "233  [1194, 861, 10, 1295, 671, 1214, 723, 917, 957...   \n",
       "234     [163, 10, 1298, 393, 888, 1118, 436, 343, 207]   \n",
       "\n",
       "                                             POS_index  \\\n",
       "0    [2, 2, 8, 7, 3, 16, 9, 16, 11, 11, 1, 8, 7, 16...   \n",
       "1    [7, 10, 2, 16, 1, 5, 11, 12, 16, 7, 16, 11, 13...   \n",
       "2    [1, 7, 1, 5, 7, 12, 11, 16, 10, 3, 16, 10, 1, ...   \n",
       "3    [14, 10, 3, 0, 1, 5, 7, 5, 3, 3, 16, 1, 11, 11...   \n",
       "4    [1, 5, 7, 12, 11, 3, 3, 16, 10, 7, 1, 5, 5, 7,...   \n",
       "..                                                 ...   \n",
       "230                        [10, 3, 16, 5, 7, 5, 7, 12]   \n",
       "231                                               [14]   \n",
       "232  [5, 7, 2, 16, 5, 7, 1, 0, 7, 13, 0, 11, 7, 11,...   \n",
       "233  [5, 7, 12, 5, 16, 9, 16, 0, 7, 4, 7, 12, 16, 1...   \n",
       "234                   [11, 12, 10, 3, 9, 16, 1, 5, 11]   \n",
       "\n",
       "                                                 Label  \n",
       "0    [W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, ...  \n",
       "1    [W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, ...  \n",
       "2    [W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, ...  \n",
       "3        [W, W, W, W, W, W, W, W, W, W, W, W, W, W, W]  \n",
       "4    [W, W, W, W, W, W, W, W, W, W, W, W, W, W, W, ...  \n",
       "..                                                 ...  \n",
       "230                           [M, M, M, M, M, M, M, M]  \n",
       "231                                                [M]  \n",
       "232  [M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, ...  \n",
       "233  [M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, ...  \n",
       "234                        [M, M, M, M, M, M, M, M, M]  \n",
       "\n",
       "[235 rows x 9 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_POS = len(POS_vocab)\n",
    "max_lemma = len(lemma_vocab)\n",
    "\n",
    "tokens = df_grouped['Lemma_index'].tolist()\n",
    "POS = df_grouped['POS_index'].tolist()\n",
    "\n",
    "max_length = max([len(s) for s in tokens])\n",
    "\n",
    "pad_tokens = pad_sequences(tokens, maxlen=max_length, dtype='int32', padding='post', value= max_lemma - 1)\n",
    "\n",
    "pad_POS = pad_sequences(POS, maxlen=max_length, dtype='int32', padding='post', value= POS_to_i[\"X\"])\n",
    "n_POS = len(POS_to_i)\n",
    "\n",
    "pad_POS = [to_categorical(i, num_classes=n_POS) for i in pad_POS]\n",
    "\n",
    "pad_labels = pad_sequences(tokens, maxlen=max_length, dtype='int32', padding='post', value= max_lemma - 1)\n",
    "\n",
    "tokens_, test_tokens, POS_, test_POS = train_test_split(pad_tokens, pad_POS, test_size=None, random_state=0)\n",
    "train_tokens, test_tokens, train_POS, test_POS = train_test_split(tokens_,POS_,test_size = 0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = len(lemma_vocab)+1\n",
    "output_dim = 64\n",
    "input_length = max([len(s) for s in df_grouped['Lemma_index'].tolist()])\n",
    "n_POS = len(POS_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_logdir = os.path.join(os.curdir, \"la_logs\")\n",
    "def get_run_logdir():\n",
    "    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
    "    return os.path.join(root_logdir, run_id)\n",
    "\n",
    "run_log_dir = get_run_logdir()\n",
    "run_log_dir\n",
    "\n",
    "tensorboard_cb = tf.keras.callbacks.TensorBoard(run_log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 1176, 64)          85184     \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 1176, 128)         66048     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 1176, 64)          49408     \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, 1176, 18)          1170      \n",
      "=================================================================\n",
      "Total params: 201,810\n",
      "Trainable params: 201,810\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "('You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) ', 'for plot_model/model_to_dot to work.')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hanse\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py:355: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential(\n",
    "    [\n",
    "        layers.Embedding(input_dim=input_dim, output_dim=output_dim, input_length=input_length), # Embedding vs Input layer\n",
    "        layers.Bidirectional(layers.LSTM(units=output_dim, return_sequences=True, dropout=0.2, recurrent_dropout=0.2), merge_mode = 'concat'),\n",
    "        layers.LSTM(units=output_dim, return_sequences=True, dropout=0.5, recurrent_dropout=0.5),\n",
    "        layers.TimeDistributed(layers.Dense(n_POS, activation=\"softmax\")) #Use softmax for activation HOML page 383\n",
    "    ]\n",
    ")\n",
    "\n",
    "adam = tf.keras.optimizers.Adam(lr=0.0005, beta_1=0.9, beta_2=0.999)\n",
    "\n",
    "#Hyper params to change - learning rate, number of layers and neurons, activations functions\n",
    "\n",
    "model.compile(loss='crossentropy', optimizer='adam', metrics=['accuracy']) # cross entropy loss chapter 4 HOML - categorial crossentropy because to_categorial \n",
    "model.summary()\n",
    "plot_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1/1 - 11s - loss: 2.8833 - accuracy: 0.0325\n",
      "Epoch 2/20\n",
      "1/1 - 12s - loss: 2.8450 - accuracy: 0.5638\n",
      "Epoch 3/20\n",
      "1/1 - 2s - loss: 2.8040 - accuracy: 0.5403\n",
      "Epoch 4/20\n",
      "1/1 - 2s - loss: 2.7569 - accuracy: 0.5316\n",
      "Epoch 5/20\n",
      "1/1 - 2s - loss: 2.6957 - accuracy: 0.5304\n",
      "Epoch 6/20\n",
      "1/1 - 2s - loss: 2.6108 - accuracy: 0.5304\n",
      "Epoch 7/20\n",
      "1/1 - 2s - loss: 2.4965 - accuracy: 0.5304\n",
      "Epoch 8/20\n",
      "1/1 - 2s - loss: 2.3332 - accuracy: 0.5304\n",
      "Epoch 9/20\n",
      "1/1 - 2s - loss: 2.1281 - accuracy: 0.5304\n",
      "Epoch 10/20\n",
      "1/1 - 2s - loss: 1.9001 - accuracy: 0.5304\n",
      "Epoch 11/20\n",
      "1/1 - 2s - loss: 1.7091 - accuracy: 0.5304\n",
      "Epoch 12/20\n",
      "1/1 - 2s - loss: 1.6014 - accuracy: 0.5304\n",
      "Epoch 13/20\n",
      "1/1 - 2s - loss: 1.5774 - accuracy: 0.5304\n",
      "Epoch 14/20\n",
      "1/1 - 2s - loss: 1.5683 - accuracy: 0.5304\n",
      "Epoch 15/20\n",
      "1/1 - 2s - loss: 1.5023 - accuracy: 0.5304\n",
      "Epoch 16/20\n",
      "1/1 - 2s - loss: 1.4346 - accuracy: 0.5310\n",
      "Epoch 17/20\n",
      "1/1 - 2s - loss: 1.4068 - accuracy: 0.5679\n",
      "Epoch 18/20\n",
      "1/1 - 2s - loss: 1.3945 - accuracy: 0.6010\n",
      "Epoch 19/20\n",
      "1/1 - 3s - loss: 1.3857 - accuracy: 0.5662\n",
      "Epoch 20/20\n",
      "1/1 - 3s - loss: 1.3781 - accuracy: 0.5575\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ERROR: Could not find `tensorboard`. Please ensure that your PATH\n",
       "contains an executable `tensorboard` program, or explicitly specify\n",
       "the path to a TensorBoard binary by setting the `TENSORBOARD_BINARY`\n",
       "environment variable."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "history = model.fit(train_tokens, np.array(train_POS), epochs=20, callbacks=[tensorboard_cb], verbose=2)\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=./la_logs --port=6006"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5ce0bbaaba556c243eee45087b7684ce43fda2aa9a5b0798832d1be21bf73af7"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit (windows store)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

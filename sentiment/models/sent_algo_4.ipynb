{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment algorithm 3. Sequence of lemmas and POS-tag as multiple features input and multi sentiment classification of sentence as output\n",
    "\n",
    "# Model is saved in saved_models/model_name/model_variant.h5\n",
    "# Run history is saved in logged_models/model_name sorted by model_variants and run-datetime\n",
    "# Runs can be viewed using tensorboard: tensorboard --logdir=PATH --port=6006\n",
    "# Example given: tensorboard --logdir=C:\\BAC\\Ezra\\sentiment\\models\\logged_models\\sent_algo_1 --port=6006\n",
    "model_name = 'sent_algo_4'\n",
    "model_variant = 'base'\n",
    "\n",
    "training_size = 3000000\n",
    "test_size = 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETUP IMPORTS\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.utils.vis_utils import plot_model\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from keras import callbacks\n",
    "from keras import Input\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from helpers.sentiment_functions import gender_seq_to_single\n",
    "from helpers.sentiment_functions import equal_genders\n",
    "from helpers.sentiment_functions import encode_feature\n",
    "from helpers.sentiment_functions import plot_sentence_lengths\n",
    "from helpers.sentiment_functions import plot_confusion_matrix_multi\n",
    "from helpers.sentiment_functions import get_metrics\n",
    "from helpers.sentiment_functions import round_list\n",
    "from helpers.sentiment_functions import encode_list\n",
    "from helpers.sentiment_functions import one_hot_list\n",
    "from helpers.sentiment_functions import xai_multi\n",
    "\n",
    "pd.set_option('display.max_columns', 10, 'display.width', 10, 'display.max_colwidth', 20, 'display.max_rows',100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD DATASET\n",
    "df = pd.read_json('../datasets/sentiment_corpus.json')\n",
    "print(\"Corpus sample size is:\", len(df))\n",
    "\n",
    "print(training_size, \"samples are taken from the head for training\")\n",
    "print(test_size, \"samples are taken from the tail for test\")\n",
    "\n",
    "# We take from the head for training data and tail for test data\n",
    "# This is done since the last 25% of the corpus is not fitted on the polarity dict, thereby preventing overfitting\n",
    "train_data = df.head(training_size)\n",
    "test_data = df.tail(test_size)\n",
    "\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GROUP-BY SENTENCE NUMBER \n",
    "train_data = train_data.groupby(['Sentence #'],as_index=False)['Word', 'Lemma', 'POS', 'Polarity', 'Gender'].agg(lambda x: list(x))\n",
    "test_data = test_data.groupby(['Sentence #'],as_index=False)['Word', 'Lemma', 'POS', 'Polarity', 'Gender'].agg(lambda x: list(x))\n",
    "\n",
    "train_data['Gender'] = gender_seq_to_single(train_data['Gender'])\n",
    "test_data['Gender'] = gender_seq_to_single(test_data['Gender'])\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHUFFLE TRAINING AND TEST DATA\n",
    "train_data = train_data.sample(frac=1).reset_index(drop=True)\n",
    "test_data = test_data.sample(frac=1).reset_index(drop=True)\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EQUAL GENDER SAMPLES TO PREVENT BIAS\n",
    "train_data = equal_genders(train_data)\n",
    "test_data = equal_genders(test_data)\n",
    "\n",
    "train_data[\"Gender\"].value_counts().plot(kind=\"bar\", figsize=(10,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALIZE SENTENCE LENGTHS AND DECISION BOUNDARY\n",
    "decision_boundary = plot_sentence_lengths(train_data)\n",
    "print(\"Decision boundary / 80 pct of sentence lengths is:\", decision_boundary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONE-HOT TARGET (GENDER)\n",
    "classes = [-1.0, -0.9, -0.8, -0.7, -0.6, -0.5, -0.4, -0.3, -0.2, -0.1, 0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "n_unique_classes = len(classes)\n",
    "\n",
    "pol_to_enc = {\n",
    "    -1.0: 1,\n",
    "    -0.9: 2,\n",
    "    -0.8: 3,\n",
    "    -0.7: 4,\n",
    "    -0.6: 5,\n",
    "    -0.5: 6,\n",
    "    -0.4: 7,\n",
    "    -0.3: 8,\n",
    "    -0.2: 9,\n",
    "    -0.1: 10,\n",
    "    0.0: 11,\n",
    "    0.1: 12,\n",
    "    0.2: 13,\n",
    "    0.3: 14,\n",
    "    0.4: 15,\n",
    "    0.5: 16,\n",
    "    0.6: 17,\n",
    "    0.7: 18,\n",
    "    0.8: 19,\n",
    "    0.9: 20,\n",
    "    1.0: 21\n",
    "}\n",
    "\n",
    "train_pols_rounded = round_list(train_data['Polarity'])\n",
    "train_pols_encoded = encode_list(train_pols_rounded, pol_to_enc)\n",
    "y_train = one_hot_list(train_pols_encoded, n_unique_classes)\n",
    "\n",
    "test_pols_rounded = round_list(test_data['Polarity'])\n",
    "test_pols_encoded = encode_list(test_pols_rounded, pol_to_enc)\n",
    "y_test = one_hot_list(test_pols_encoded, n_unique_classes)\n",
    "\n",
    "\n",
    "print(\"Unique polarities:\", [classes])\n",
    "print(\"Unique polarities:\", encode_list([classes], pol_to_enc))\n",
    "\n",
    "print(\"\\nTraining data example polarity sequence:\", train_data['Polarity'][:1].tolist())\n",
    "print(\"Training data example polarity sequence rounded:\", train_pols_rounded[0])\n",
    "print(\"Training data example polarity sequence encoded:\", train_pols_encoded[0])\n",
    "print(\"Training data example polarity sequence one-hot:\", y_train[0])\n",
    "\n",
    "print(\"\\nTest data example polarity sequence:\", test_data['Polarity'][:1].tolist())\n",
    "print(\"Test data example polarity sequence rounded:\", test_pols_rounded[0])\n",
    "print(\"Test data example polarity sequence encoded:\", test_pols_encoded[0])\n",
    "print(\"Test data example polarity sequence one-hot:\", y_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENCODE FEATURES TO INTEGERS, EQUAL LENGTHS AND PAD\n",
    "# Inspired by: https://machinelearningmastery.com/prepare-text-data-deep-learning-keras/\n",
    "train_data['Lemma_enc'], test_data['Lemma_enc'], vocab_size_lemma = encode_feature(train_data['Lemma'], test_data['Lemma'])\n",
    "train_data['POS_enc'], test_data['POS_enc'], vocab_size_pos = encode_feature(train_data['POS'], test_data['POS'])\n",
    "\n",
    "trunc_type = 'post'\n",
    "padding_type = 'post'\n",
    "max_len = decision_boundary\n",
    "\n",
    "X_train_lemma = pad_sequences(train_data['Lemma_enc'], dtype='float32', padding=padding_type, truncating=trunc_type, maxlen=max_len)\n",
    "X_train_pos = pad_sequences(train_data['POS_enc'], dtype='float32', padding=padding_type, truncating=trunc_type, maxlen=max_len)\n",
    "\n",
    "X_test_lemma = pad_sequences(test_data['Lemma_enc'], dtype='float32', padding=padding_type, truncating=trunc_type, maxlen=max_len)\n",
    "X_test_pos = pad_sequences(test_data['POS_enc'], dtype='float32', padding=padding_type, truncating=trunc_type, maxlen=max_len)\n",
    "\n",
    "y_train = pad_sequences(y_train, dtype='float32', padding=padding_type, truncating=trunc_type, maxlen=max_len)\n",
    "y_test = pad_sequences(y_test, dtype='float32', padding=padding_type, truncating=trunc_type, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINING KERAS CALLBACKS\n",
    "\n",
    "# Borrowed from: https://www.geeksforgeeks.org/choose-optimal-number-of-epochs-to-train-a-neural-network-in-keras/\n",
    "earlystopping = callbacks.EarlyStopping(monitor =\"val_loss\", \n",
    "                                        mode =\"min\", patience = 5, \n",
    "                                        restore_best_weights = True)\n",
    "\n",
    "root_logdir = os.path.join(os.curdir, \"./logged_models/\" + model_name + '/' + model_variant)\n",
    "def get_run_logdir():\n",
    "    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
    "    return os.path.join(root_logdir, run_id)\n",
    "\n",
    "run_log_dir = get_run_logdir()\n",
    "file_writer = tf.summary.create_file_writer(run_log_dir + \"/metrics\")\n",
    "file_writer.set_as_default()\n",
    "\n",
    "tensorboard_cb = tf.keras.callbacks.TensorBoard(run_log_dir)\n",
    "\n",
    "my_callbacks = [earlystopping, tensorboard_cb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINING AND PLOTTING MODEL\n",
    "embedding_dim = 128\n",
    "\n",
    "lemma_input = Input(shape=(max_len), name='lemma_input')\n",
    "pos_input = Input(shape=(max_len), name='POS_input')\n",
    "\n",
    "lemma_embedding = layers.Embedding(vocab_size_lemma+1, embedding_dim, input_length=max_len, name=\"lemma_embedding\", mask_zero=True)(lemma_input)\n",
    "pos_embedding = layers.Embedding(vocab_size_pos+1, embedding_dim, input_length=max_len, name=\"POS_embedding\")(pos_input)\n",
    "\n",
    "concat = layers.Concatenate()([lemma_embedding, pos_embedding])\n",
    "\n",
    "bidir_1 = layers.Bidirectional(layers.LSTM(128, return_sequences=True, dropout=0.3, recurrent_dropout=0.3))(concat)\n",
    "\n",
    "bidir_2 = layers.Bidirectional(layers.LSTM(64, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(bidir_1)\n",
    "\n",
    "output = layers.TimeDistributed(layers.Dense(n_unique_classes, activation=\"softmax\", name='output'))(bidir_2)\n",
    "\n",
    "model = keras.Model(inputs=[lemma_input, pos_input], outputs=[output])\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) # cross entropy loss chapter 4 HOML - categorial crossentropy because to_categorial \n",
    "\n",
    "model.summary()\n",
    "\n",
    "plot_model(model, show_shapes=True, show_layer_names=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FITTING MODEL\n",
    "\n",
    "history = model.fit([X_train_lemma, X_train_pos], y_train, epochs=20, callbacks=my_callbacks, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOTTING HISTORY OF FITTING\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs=range(len(acc))\n",
    "plt.plot(epochs, acc, 'r', 'Training Accuracy')\n",
    "plt.plot(epochs, val_acc, 'b', 'Validation Accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'r', 'Training Loss')\n",
    "plt.plot(epochs, val_loss, 'b', 'Validation Loss')\n",
    "plt.title('Training and validation loss')\n",
    "\n",
    "print(\"red is training, blue is validation\")\n",
    "plt.figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVING MODEL\n",
    "model.save('saved_models/' + model_name + '/' + model_variant + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREDICT TEST DATA\n",
    "y_pred = model.predict([X_test_lemma, X_test_pos])\n",
    "cm_pred = y_pred.argmax(axis=-1).flatten()\n",
    "cm_true = y_test.argmax(axis=-1).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for seq in test_data['Polarity'].to_numpy():\n",
    "    count += len(seq)\n",
    "count\n",
    "print(\"test_data polarities length\", count)\n",
    "print(\"cm_pred shape\", cm_pred.shape)\n",
    "print(\"cm_true shape\", cm_true.shape)\n",
    "print(\"MEANING\", cm_pred.shape[0]-count, \"polarities are paddings\")\n",
    "print(cm_pred[:50])\n",
    "print(cm_true[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOTTING CONFUSION MATRIX\n",
    "enc_to_pol = {\n",
    "  0: \"PAD\",\n",
    "  1 : -1.0,\n",
    "  2 : -0.9,\n",
    "  3 : -0.8,\n",
    "  4 : -0.7,\n",
    "  5 : -0.6,\n",
    "  6 : -0.5,\n",
    "  7 : -0.4,\n",
    "  8 : -0.3,\n",
    "  9 : -0.2,\n",
    "  10 : -0.1,\n",
    "  11 : 0.0,\n",
    "  12 : 0.1,\n",
    "  13 : 0.2,\n",
    "  14 : 0.3,\n",
    "  15 : 0.4,\n",
    "  16 : 0.5,\n",
    "  17 : 0.6,\n",
    "  18 : 0.7,\n",
    "  19 : 0.8,\n",
    "  20 : 0.9,\n",
    "  21 : 1.0,\n",
    "}\n",
    "\n",
    "plot_confusion_matrix_multi(cm_true, cm_pred, 'Multi sentiment classification', 'Predicted polarity', 'True polarity', enc_to_pol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRINTING METRICS\n",
    "present_polarities = []\n",
    "\n",
    "for x in np.unique(np.array(np.concatenate((cm_true,cm_pred)))):\n",
    "    present_polarities.append(enc_to_pol[x])\n",
    "\n",
    "get_metrics(cm_true, cm_pred, present_polarities, 'Polarity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USE XAI ON PREDICTIONS\n",
    "xai_multi(y_pred.argmax(axis=-1)[:500], test_data, 2, enc_to_pol)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5ce0bbaaba556c243eee45087b7684ce43fda2aa9a5b0798832d1be21bf73af7"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

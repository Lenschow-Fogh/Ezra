{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment algorithm 1. Sequence of lemmas as single feature input and binary sentiment classification of sentence as output\n",
    "\n",
    "# Model is saved in saved_models/model_name/model_variant.h5\n",
    "# Run history is saved in logged_models/model_name sorted by model_variants and run-datetime\n",
    "# Runs can be viewed using tensorboard: tensorboard --logdir=PATH --port=6006\n",
    "# Example given: tensorboard --logdir=C:\\BAC\\Ezra\\sentiment\\models\\logged_models\\sent_algo_1 --port=6006\n",
    "model_name = 'sent_algo_1'\n",
    "model_variant = 'base'\n",
    "\n",
    "training_size = 3000000\n",
    "test_size = 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETUP IMPORTS\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "import pandas as pd\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from keras.utils.vis_utils import plot_model\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import os\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from keras import callbacks\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from helpers.sentiment_functions import gender_seq_to_single\n",
    "from helpers.sentiment_functions import equal_genders\n",
    "from helpers.sentiment_functions import encode_feature\n",
    "from helpers.sentiment_functions import plot_sentence_lengths\n",
    "from helpers.sentiment_functions import plot_confusion_matrix_binary\n",
    "from helpers.sentiment_functions import get_metrics\n",
    "from helpers.sentiment_functions import xai_binary\n",
    "\n",
    "pd.set_option('display.max_columns', 10, 'display.width', 15, 'display.max_colwidth', 15, 'display.max_rows',100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD DATASET\n",
    "df = pd.read_json('../datasets/sentiment_corpus.json')\n",
    "print(\"Corpus sample size is:\", len(df))\n",
    "\n",
    "print(training_size, \"samples are taken from the head for training\")\n",
    "print(test_size, \"samples are taken from the tail for test\")\n",
    "\n",
    "# We take from the head for training data and tail for test data\n",
    "# This is done since the last 25% of the corpus is not fitted on the polarity dict, thereby preventing overfitting\n",
    "train_data = df.head(training_size)\n",
    "test_data = df.tail(test_size)\n",
    "\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GROUP-BY SENTENCE NUMBER \n",
    "train_data = train_data.groupby(['Sentence #'],as_index=False)['Word', 'Lemma', 'POS', 'Polarity', 'Gender'].agg(lambda x: list(x))\n",
    "test_data = test_data.groupby(['Sentence #'],as_index=False)['Word', 'Lemma', 'POS', 'Polarity', 'Gender'].agg(lambda x: list(x))\n",
    "\n",
    "train_data['Gender'] = gender_seq_to_single(train_data['Gender'])\n",
    "test_data['Gender'] = gender_seq_to_single(test_data['Gender'])\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHUFFLE TRAINING AND TEST DATA\n",
    "train_data = train_data.sample(frac=1).reset_index(drop=True)\n",
    "test_data = test_data.sample(frac=1).reset_index(drop=True)\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EQUAL GENDER SAMPLES TO PREVENT BIAS\n",
    "train_data = equal_genders(train_data)\n",
    "test_data = equal_genders(test_data)\n",
    "\n",
    "train_data[\"Gender\"].value_counts().plot(kind=\"bar\", figsize=(10,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALIZE SENTENCE LENGTHS AND DECISION BOUNDARY\n",
    "decision_boundary = plot_sentence_lengths(train_data)\n",
    "print(\"Decision boundary / 80 pct of sentence lengths is:\", decision_boundary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENCODE FEATURES TO INTEGERS, EQUAL LENGTHS AND PAD\n",
    "# Inspired by: https://machinelearningmastery.com/prepare-text-data-deep-learning-keras/\n",
    "train_data['Lemma_enc'], test_data['Lemma_enc'], vocab_size = encode_feature(train_data['Lemma'], test_data['Lemma'])\n",
    "\n",
    "trunc_type = 'post'\n",
    "padding_type = 'post'\n",
    "max_len = decision_boundary\n",
    "\n",
    "X_train = pad_sequences(train_data['Lemma_enc'], dtype='float32', padding=padding_type, truncating=trunc_type, maxlen=max_len)\n",
    "X_test = pad_sequences(test_data['Lemma_enc'], dtype='float32', padding=padding_type, truncating=trunc_type, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONE-HOT TARGET (GENDER)\n",
    "le = LabelEncoder()\n",
    "le.classes_ = ['M', 'F']\n",
    "\n",
    "y_train = le.transform(train_data['Gender'])\n",
    "y_test = le.transform(test_data['Gender'])\n",
    "\n",
    "y_train = to_categorical(y_train)\n",
    "\n",
    "print('[M] =', le.transform(['M']), '=', to_categorical(le.transform(['M']),2))\n",
    "print('[F] =', le.transform(['F']), '=', to_categorical(le.transform(['F']),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINING KERAS CALLBACKS\n",
    "\n",
    "# Borrowed from: https://www.geeksforgeeks.org/choose-optimal-number-of-epochs-to-train-a-neural-network-in-keras/\n",
    "earlystopping = callbacks.EarlyStopping(monitor =\"val_loss\", \n",
    "                                        mode =\"min\", patience = 5, \n",
    "                                        restore_best_weights = True)\n",
    "\n",
    "root_logdir = os.path.join(os.curdir, \"./logged_models/\" + model_name + '/' + model_variant)\n",
    "def get_run_logdir():\n",
    "    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
    "    return os.path.join(root_logdir, run_id)\n",
    "\n",
    "run_log_dir = get_run_logdir()\n",
    "file_writer = tf.summary.create_file_writer(run_log_dir + \"/metrics\")\n",
    "file_writer.set_as_default()\n",
    "\n",
    "tensorboard_cb = tf.keras.callbacks.TensorBoard(run_log_dir)\n",
    "\n",
    "my_callbacks = [earlystopping, tensorboard_cb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINING AND PLOTTING MODEL\n",
    "embedding_dim = 128\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Embedding(vocab_size+1, embedding_dim, input_length=max_len, mask_zero=True),\n",
    "    layers.Bidirectional(layers.LSTM(128, return_sequences=True, dropout=0.3, recurrent_dropout=0.3)),\n",
    "    layers.Bidirectional(layers.LSTM(64, return_sequences=False, dropout=0.1, recurrent_dropout=0.1)),\n",
    "    layers.Dense(2, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) # cross entropy loss chapter 4 HOML - categorial crossentropy because to_categorial \n",
    "\n",
    "model.summary()\n",
    "\n",
    "plot_model(model, show_shapes=True, show_layer_names=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FITTING MODEL\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=20, callbacks=my_callbacks, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOTTING HISTORY OF FITTING\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs=range(len(acc))\n",
    "plt.plot(epochs, acc, 'r', 'Training Accuracy')\n",
    "plt.plot(epochs, val_acc, 'b', 'Validation Accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'r', 'Training Loss')\n",
    "plt.plot(epochs, val_loss, 'b', 'Validation Loss')\n",
    "plt.title('Training and validation loss')\n",
    "\n",
    "print(\"red is training, blue is validation\")\n",
    "plt.figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVING MODEL\n",
    "model.save('saved_models/' + model_name + '/' + model_variant + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREDICT TEST DATA\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = y_pred.argmax(axis=-1)\n",
    "y_pred = y_pred.flatten()\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOTTING CONFUSION MATRIX\n",
    "plot_confusion_matrix_binary(np.array(y_test).flatten(), y_pred, 'Binary sentiment classification', 'Predicted gender', 'True gender')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRINTING METRICS\n",
    "sentiment_vocab = ['M', 'F']\n",
    "get_metrics(y_test, y_pred, sentiment_vocab, 'Gender')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USE XAI ON PREDICTIONS\n",
    "xai_binary(y_pred[:100], test_data, 2)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5ce0bbaaba556c243eee45087b7684ce43fda2aa9a5b0798832d1be21bf73af7"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment algorithm 1. Sequence of lemmas as single feature input and binary classification of sentence as output\n",
    "\n",
    "# Model is saved in saved_models/model_name/model_variant.h5\n",
    "# Run history is saved in logged_models/model_name sorted by model_variants and run-datetime\n",
    "# Runs can be viewed using tensorboard: tensorboard --logdir=PATH --port=6006\n",
    "# Example given: tensorboard --logdir=C:\\BAC\\Ezra\\sentiment\\models\\logged_models\\sent_algo_1 --port=6006\n",
    "model_name = 'sent_algo_1'\n",
    "model_variant = 'base'\n",
    "\n",
    "training_size = 3000000\n",
    "test_size = 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETUP IMPORTS\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.utils.vis_utils import plot_model\n",
    "import time\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics import classification_report\n",
    "from collections import Counter\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras import models\n",
    "import os\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from keras import callbacks\n",
    "pd.set_option('display.max_columns', 10, 'display.width', 10, 'display.max_colwidth', 20, 'display.max_rows',100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD DATASET\n",
    "df = pd.read_json('../datasets/sentiment_corpus.json')\n",
    "print(\"Corpus sample size is:\", len(df))\n",
    "\n",
    "print(training_size, \"samples are taken from the head for training\")\n",
    "print(test_size, \"samples are taken from the tail for test\")\n",
    "\n",
    "# We take from the head for training data and tail for test data\n",
    "# This is done since the last 25% of the corpus is not fitted on the polarity dict, thereby preventing overfitting\n",
    "train_data = df.head(training_size)\n",
    "test_data = df.tail(test_size)\n",
    "\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GROUP-BY SENTENCE NUMBER \n",
    "train_data = train_data.groupby(['Sentence #'],as_index=False)['Word', 'Lemma', 'POS', 'Polarity', 'Gender'].agg(lambda x: list(x))\n",
    "test_data = test_data.groupby(['Sentence #'],as_index=False)['Word', 'Lemma', 'POS', 'Polarity', 'Gender'].agg(lambda x: list(x))\n",
    "\n",
    "def gender_seq_to_single(seqs):\n",
    "    genders = []\n",
    "    for seq in seqs:\n",
    "        genders.append(seq[0])\n",
    "    return genders\n",
    "\n",
    "train_data['Gender'] = gender_seq_to_single(train_data['Gender'])\n",
    "test_data['Gender'] = gender_seq_to_single(test_data['Gender'])\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EQUAL GENDER SAMPLES TO PREVENT BIAS\n",
    "def equal_genders(data_):\n",
    "    M, F = data_[\"Gender\"].value_counts()\n",
    "    if M>F:\n",
    "        diff = M-F\n",
    "        return data_.drop(data_.loc[data_['Gender'] == 'M'].index[:diff], axis=0)\n",
    "    elif F>M:\n",
    "        diff = F-M\n",
    "        return data_.drop(data_.loc[data_['Gender'] == 'F'].index[:diff], axis=0)\n",
    "\n",
    "train_data = equal_genders(train_data)\n",
    "test_data = equal_genders(test_data)\n",
    "\n",
    "train_data[\"Gender\"].value_counts().plot(kind=\"bar\", figsize=(10,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHUFFLE TRAINING AND TEST DATA\n",
    "train_data = train_data.sample(frac=1).reset_index(drop=True)\n",
    "test_data = test_data.sample(frac=1).reset_index(drop=True)\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE FUNCTION FOR PLOTTING SENTENCE LENGTHS\n",
    "def plot_sentence_lengths(data_):\n",
    "    sentence_plot = data_[\"Word\"].values\n",
    "    sentence_plot_sorted = list(sorted(sentence_plot, key=len))\n",
    "    c = Counter(map(len, sentence_plot_sorted))\n",
    "\n",
    "    total_sentences = 0\n",
    "    total_words = 0\n",
    "    for i in c:\n",
    "        total_sentences = total_sentences + c[i]\n",
    "        total_words = total_words + c[i]*i\n",
    "\n",
    "    sentences_80_pct = total_sentences / 100 * 90\n",
    "    words_80_pct = total_words / 100 * 90\n",
    "\n",
    "    boundary_sen = 0\n",
    "    counter_sen = 0\n",
    "\n",
    "    for i in c:\n",
    "        if(counter_sen + c[i] < int(sentences_80_pct)):\n",
    "            counter_sen = counter_sen + c[i]\n",
    "            boundary_sen = i\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    boundary_word = 0\n",
    "    counter_word = 0\n",
    "\n",
    "    for i in c:\n",
    "        if(counter_word + c[i] * i < int(words_80_pct)):\n",
    "            counter_word = counter_word + c[i] * i\n",
    "            boundary_word = i\n",
    "        else:\n",
    "            break\n",
    "\n",
    "\n",
    "    my_cmap = plt.get_cmap(\"viridis\")\n",
    "    rescale = lambda y: (y - np.min(y)) / (np.max(y) - np.min(y))\n",
    "\n",
    "\n",
    "    plt.figure(figsize=[10,6])\n",
    "    bars = plt.bar(list(c.keys()), list(c.values()), color=my_cmap(rescale(list(c.values()))), width=0.8, alpha=0.7, align='center')\n",
    "\n",
    "    # for r in bars.get_children():\n",
    "    #     if(r.get_x() > boundary_sen):\n",
    "    #         r.set_alpha(0.2)\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.ylim([0, max(list(c.values()))+10])\n",
    "    ax2 = plt.gca()\n",
    "\n",
    "    ymin, ymax = ax2.get_ylim()\n",
    "    plt.vlines(boundary_sen, ymin=ymin, ymax=ymax, colors='r', label='80% of sentences')\n",
    "    # plt.vlines(boundary_word, ymin=ymin, ymax=ymax, colors='black', label=\"80% of words\")\n",
    "\n",
    "    plt.ylabel('Frequency of sentence', fontdict={'fontsize':13, 'fontweight': 'bold'})\n",
    "    plt.xlabel('# words in sentence', fontdict={'fontsize':13, 'fontweight': 'bold'})\n",
    "    plt.title(\"Distribution of sentence lengths\", fontdict={'fontsize':14, 'fontweight': 'bold'})\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return boundary_sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALIZE SENTENCE LENGTHS AND DECISION BOUNDARY\n",
    "decision_boundary = plot_sentence_lengths(train_data)\n",
    "print(\"Decision boundary / 80 pct of sentence lengths is:\", decision_boundary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENCODE FEATURES TO INTEGERS, EQUAL LENGTHS AND PAD\n",
    "# Inspired by: https://machinelearningmastery.com/prepare-text-data-deep-learning-keras/\n",
    "def encode_feature(train_data_, test_data_):\n",
    "    tokenizer = Tokenizer()\n",
    "    # ONLY FIT ON TRAIN DATA\n",
    "    tokenizer.fit_on_texts(train_data_)\n",
    "    return tokenizer.texts_to_sequences(train_data_), tokenizer.texts_to_sequences(test_data_), len(tokenizer.word_index)\n",
    "\n",
    "train_data['Lemma_enc'], test_data['Lemma_enc'], vocab_size = encode_feature(train_data['Lemma'], test_data['Lemma'])\n",
    "\n",
    "\n",
    "trunc_type = 'post'\n",
    "padding_type = 'post'\n",
    "max_len = decision_boundary\n",
    "\n",
    "X_train = pad_sequences(train_data['Lemma_enc'], dtype='float32', padding=padding_type, truncating=trunc_type, maxlen=max_len)\n",
    "X_test = pad_sequences(test_data['Lemma_enc'], dtype='float32', padding=padding_type, truncating=trunc_type, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONE-HOT TARGET (GENDER)\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(train_data['Gender'])\n",
    "y_test = le.transform(test_data['Gender'])\n",
    "\n",
    "y_train = to_categorical(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINING KERAS CALLBACKS\n",
    "\n",
    "# Borrowed from: https://www.geeksforgeeks.org/choose-optimal-number-of-epochs-to-train-a-neural-network-in-keras/\n",
    "earlystopping = callbacks.EarlyStopping(monitor =\"val_loss\", \n",
    "                                        mode =\"min\", patience = 5, \n",
    "                                        restore_best_weights = True)\n",
    "\n",
    "root_logdir = os.path.join(os.curdir, \"./logged_models/\" + model_name + '/' + model_variant)\n",
    "def get_run_logdir():\n",
    "    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
    "    return os.path.join(root_logdir, run_id)\n",
    "\n",
    "run_log_dir = get_run_logdir()\n",
    "file_writer = tf.summary.create_file_writer(run_log_dir + \"/metrics\")\n",
    "file_writer.set_as_default()\n",
    "\n",
    "tensorboard_cb = tf.keras.callbacks.TensorBoard(run_log_dir)\n",
    "\n",
    "my_callbacks = [earlystopping, tensorboard_cb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINING AND FITTING MODEL\n",
    "embedding_dim = 128\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Embedding(vocab_size+1, embedding_dim, input_length=max_len),\n",
    "    layers.Bidirectional(layers.LSTM(128, return_sequences=True, dropout=0.3, recurrent_dropout=0.3)),\n",
    "    layers.Bidirectional(layers.LSTM(64, return_sequences=False, dropout=0.1, recurrent_dropout=0.1)),\n",
    "    layers.Dense(2, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) # cross entropy loss chapter 4 HOML - categorial crossentropy because to_categorial \n",
    "\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=20, callbacks=my_callbacks, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOTTING HISTORY OF FITTING\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs=range(len(acc))\n",
    "plt.plot(epochs, acc, 'r', 'Training Accuracy')\n",
    "plt.plot(epochs, val_acc, 'b', 'Validation Accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'r', 'Training Loss')\n",
    "plt.plot(epochs, val_loss, 'b', 'Validation Loss')\n",
    "plt.title('Training and validation loss')\n",
    "\n",
    "print(\"red is training, blue is validation\")\n",
    "plt.figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVING MODEL\n",
    "model.save('saved_models/' + model_name + '/' + model_variant + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOTTING CONFUSION MATRIX\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = y_pred.argmax(axis=-1)\n",
    "\n",
    "y_test = np.array(y_test).flatten()\n",
    "y_pred = y_pred.flatten()\n",
    "cm = confusion_matrix(y_test, y_pred, normalize='true')\n",
    "\n",
    "fig = plt.figure( figsize=[18.5,10.5])\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(cm)\n",
    "ax.set_xlabel('Predicted gender', fontsize = 15, labelpad=15.0)\n",
    "ax.xaxis.set_label_position('top')\n",
    "ax.set_ylabel('True gender', fontsize = 15, labelpad=15.0)\n",
    "ax.set_title('Binary sentiment analysis range: 0, 1 / 0, 1',fontweight=\"bold\", size=20, pad=100.0)\n",
    "\n",
    "cm_axis_vals = []\n",
    "\n",
    "for x in np.unique(np.array(y_pred)):\n",
    "    cm_axis_vals.append(x)\n",
    "\n",
    "fig.colorbar(cax)\n",
    "plt.xticks(range(2), cm_axis_vals, rotation=90)\n",
    "plt.yticks(range(2), cm_axis_vals)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRINTING METRICS\n",
    "from sklearn.metrics import classification_report\n",
    "sentiment_vocab = ['M', 'F']\n",
    "\n",
    "report = classification_report(y_test, y_pred, output_dict=True)\n",
    "df_perf = pd.DataFrame.from_dict(report).transpose()\n",
    "df_perf_2 = df_perf[:2]\n",
    "df_perf_2.insert(loc=0, column='Gender', value=sentiment_vocab)\n",
    "df_perf_2.precision = df_perf_2.precision.round(2)\n",
    "df_perf_2.recall = df_perf_2.recall.round(2)\n",
    "df_perf_2['f1-score'] = df_perf_2['f1-score'].round(2)\n",
    "df_perf_2.support = df_perf_2.support.round()\n",
    "df_perf_2.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "df_perf_2"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5ce0bbaaba556c243eee45087b7684ce43fda2aa9a5b0798832d1be21bf73af7"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

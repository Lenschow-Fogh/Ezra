{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTANTIATE NLP FROM SPACY\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base f: ['Jane Smalls is prettier than pretty John', 'I am a very supporting person', 'I thrive in a nurturing setting']\n",
      "lemmatized m: [['be', 'strong', 'than', 'strong'], ['I', 'be', 'a', 'very', 'strong', 'person'], ['I', 'thrive', 'in', 'a', 'competitive', 'setting']]\n",
      "lemmatized f: [['be', 'pretty', 'than', 'pretty', 'John'], ['I', 'be', 'a', 'very', 'support', 'person'], ['I', 'thrive', 'in', 'a', 'nurture', 'set']]\n",
      "tf m: [[1, 2, 1, 2], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]\n",
      "tf f: [[1, 2, 1, 2, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]\n",
      "df m: {'strong': 2, 'than': 1, 'be': 2, 'I': 2, 'very': 1, 'a': 2, 'person': 1, 'in': 1, 'thrive': 1, 'setting': 1, 'competitive': 1}\n",
      "df f: {'than': 1, 'pretty': 1, 'John': 1, 'be': 2, 'support': 1, 'I': 2, 'very': 1, 'a': 2, 'person': 1, 'in': 1, 'thrive': 1, 'set': 1, 'nurture': 1}\n",
      "idf m: OrderedDict([('than', 1.79), ('very', 1.79), ('person', 1.79), ('in', 1.79), ('thrive', 1.79), ('setting', 1.79), ('competitive', 1.79), ('strong', 1.1), ('be', 1.1), ('I', 1.1), ('a', 1.1)])\n",
      "idf f: OrderedDict([('than', 1.79), ('pretty', 1.79), ('John', 1.79), ('support', 1.79), ('very', 1.79), ('person', 1.79), ('in', 1.79), ('thrive', 1.79), ('set', 1.79), ('nurture', 1.79), ('be', 1.1), ('I', 1.1), ('a', 1.1)])\n",
      "------------------------------------------\n",
      "sum tf-idf m: OrderedDict([('strong', 0.92), ('be', 0.37), ('I', 0.37), ('a', 0.37), ('than', 0.3), ('very', 0.3), ('person', 0.3), ('thrive', 0.3), ('in', 0.3), ('competitive', 0.3), ('setting', 0.3)])\n",
      "sum tf-idf f: OrderedDict([('pretty', 1.19), ('be', 0.37), ('I', 0.37), ('a', 0.37), ('than', 0.3), ('John', 0.3), ('very', 0.3), ('support', 0.3), ('person', 0.3), ('thrive', 0.3), ('in', 0.3), ('nurture', 0.3), ('set', 0.3)])\n",
      "------------------------------------------\n",
      "normalize v_min 0.3 v_max 0.92\n",
      "key strong d[k]-v_min 0.6200000000000001 v_max-v_min 0.6200000000000001 result 1.0\n",
      "key be d[k]-v_min 0.07 v_max-v_min 0.6200000000000001 result 0.11290322580645161\n",
      "key I d[k]-v_min 0.07 v_max-v_min 0.6200000000000001 result 0.11290322580645161\n",
      "key a d[k]-v_min 0.07 v_max-v_min 0.6200000000000001 result 0.11290322580645161\n",
      "key than d[k]-v_min 0.0 v_max-v_min 0.6200000000000001 result 0.0\n",
      "key very d[k]-v_min 0.0 v_max-v_min 0.6200000000000001 result 0.0\n",
      "key person d[k]-v_min 0.0 v_max-v_min 0.6200000000000001 result 0.0\n",
      "key thrive d[k]-v_min 0.0 v_max-v_min 0.6200000000000001 result 0.0\n",
      "key in d[k]-v_min 0.0 v_max-v_min 0.6200000000000001 result 0.0\n",
      "key competitive d[k]-v_min 0.0 v_max-v_min 0.6200000000000001 result 0.0\n",
      "key setting d[k]-v_min 0.0 v_max-v_min 0.6200000000000001 result 0.0\n",
      "normalize v_min 0.3 v_max 1.19\n",
      "key pretty d[k]-v_min 0.8899999999999999 v_max-v_min 0.8899999999999999 result 1.0\n",
      "key be d[k]-v_min 0.07 v_max-v_min 0.8899999999999999 result 0.07865168539325844\n",
      "key I d[k]-v_min 0.07 v_max-v_min 0.8899999999999999 result 0.07865168539325844\n",
      "key a d[k]-v_min 0.07 v_max-v_min 0.8899999999999999 result 0.07865168539325844\n",
      "key than d[k]-v_min 0.0 v_max-v_min 0.8899999999999999 result 0.0\n",
      "key John d[k]-v_min 0.0 v_max-v_min 0.8899999999999999 result 0.0\n",
      "key very d[k]-v_min 0.0 v_max-v_min 0.8899999999999999 result 0.0\n",
      "key support d[k]-v_min 0.0 v_max-v_min 0.8899999999999999 result 0.0\n",
      "key person d[k]-v_min 0.0 v_max-v_min 0.8899999999999999 result 0.0\n",
      "key thrive d[k]-v_min 0.0 v_max-v_min 0.8899999999999999 result 0.0\n",
      "key in d[k]-v_min 0.0 v_max-v_min 0.8899999999999999 result 0.0\n",
      "key nurture d[k]-v_min 0.0 v_max-v_min 0.8899999999999999 result 0.0\n",
      "key set d[k]-v_min 0.0 v_max-v_min 0.8899999999999999 result 0.0\n",
      "normalized m OrderedDict([('strong', 1.0), ('be', 0.11290322580645161), ('I', 0.11290322580645161), ('a', 0.11290322580645161), ('than', 0.0), ('very', 0.0), ('person', 0.0), ('thrive', 0.0), ('in', 0.0), ('competitive', 0.0), ('setting', 0.0)])\n",
      "----------------------------------------------------------\n",
      "polarity dict: OrderedDict([('pretty', 1.0), ('than', 0.0), ('John', 0.0), ('very', 0.0), ('support', 0.0), ('person', 0.0), ('thrive', 0.0), ('in', 0.0), ('nurture', 0.0), ('set', 0.0), ('competitive', -0.0), ('setting', -0.0), ('be', -0.03425154041319317), ('I', -0.03425154041319317), ('a', -0.03425154041319317), ('strong', -1.0)])\n"
     ]
    }
   ],
   "source": [
    "m = [\"John Johnson is stronger than strong Jane\", \"I am a very strong person\", \"I thrive in a competitive setting\"]\n",
    "f = [\"Jane Smalls is prettier than pretty John\", \"I am a very supporting person\", \"I thrive in a nurturing setting\"]\n",
    "\n",
    "print('base f:', f)\n",
    "\n",
    "#### PREPROCESSING (LEMMATIZING AND FILTERING) ####\n",
    "ignore_terms = ['.', ',', '...', ' ', '\\u2019', '  ', '(', ')', '?', '\\u00a3', '/', '\"', ':', ';', '-', '--', '\\u2015', \"'\", '!', '$', '#', '\\u2014', '   ', '[',']']\n",
    "ignore_ents = ['TIME', 'DATE', 'GPE', 'CARDINAL', 'PERSON', 'MONEY', 'PERCENT']\n",
    "\n",
    "\n",
    "m_lem = []\n",
    "f_lem = []\n",
    "\n",
    "for sentence in m:\n",
    "    doc = nlp(sentence)\n",
    "    lemmatized_sentence = []\n",
    "    for token in doc:\n",
    "        if token.lemma_ not in ignore_terms and token.ent_type_ not in ignore_ents:\n",
    "            lemmatized_sentence.append(token.lemma_)\n",
    "    m_lem.append(lemmatized_sentence)\n",
    "\n",
    "for sentence in f:\n",
    "    doc = nlp(sentence)\n",
    "    lemmatized_sentence = []\n",
    "    for token in doc:\n",
    "        if token.lemma_ not in ignore_terms and token.ent_type_ not in ignore_ents:\n",
    "            lemmatized_sentence.append(token.lemma_)\n",
    "    f_lem.append(lemmatized_sentence)\n",
    "\n",
    "print('lemmatized m:', m_lem)\n",
    "print('lemmatized f:', f_lem)\n",
    "\n",
    "#### COMPUTING TERM FREQUENCY ####\n",
    "\n",
    "m_tf = []\n",
    "f_tf = []\n",
    "for sentence in m_lem:\n",
    "    tf_d = {}\n",
    "    tf = []\n",
    "    \n",
    "    # Count term frequencies in each doc/sentence\n",
    "    for lemma in sentence:\n",
    "        if lemma in tf_d:\n",
    "            tf_d[lemma] += 1\n",
    "        else:\n",
    "            tf_d[lemma] = 1\n",
    "\n",
    "    # Append to array to show tf\n",
    "    for lemma in sentence:\n",
    "        tf.append(tf_d[lemma])\n",
    "\n",
    "    m_tf.append(tf)\n",
    "\n",
    "for sentence in f_lem:\n",
    "    tf_d = {}\n",
    "    tf = []\n",
    "    \n",
    "    # Count term frequencies in each doc/sentence\n",
    "    for lemma in sentence:\n",
    "        if lemma in tf_d:\n",
    "            tf_d[lemma] += 1\n",
    "        else:\n",
    "            tf_d[lemma] = 1\n",
    "\n",
    "    # Append to array to show tf\n",
    "    for lemma in sentence:\n",
    "        tf.append(tf_d[lemma])\n",
    "\n",
    "    f_tf.append(tf)\n",
    "\n",
    "print('tf m:', m_tf)\n",
    "print('tf f:', f_tf)\n",
    "\n",
    "#### COMPUTING DOCUMENT FREQUENCY ####\n",
    "m_df = {}\n",
    "f_df = {}\n",
    "for sentence in m_lem:\n",
    "    for unique_lemma in set(sentence):\n",
    "        if unique_lemma in m_df:\n",
    "            m_df[unique_lemma] += 1\n",
    "        else:\n",
    "            m_df[unique_lemma] = 1\n",
    "\n",
    "for sentence in f_lem:\n",
    "    for unique_lemma in set(sentence):\n",
    "        if unique_lemma in f_df:\n",
    "            f_df[unique_lemma] += 1\n",
    "        else:\n",
    "            f_df[unique_lemma] = 1\n",
    "\n",
    "print('df m:', m_df)\n",
    "print('df f:', f_df)\n",
    "\n",
    "#### COMPUTING INVERSE DOCUMENT FREQUENCY ####\n",
    "from collections import OrderedDict\n",
    "import math\n",
    "N = len(m) + len(f)\n",
    "\n",
    "m_idf = {}\n",
    "f_idf = {}\n",
    "\n",
    "for key, value in m_df.items():\n",
    "    m_idf[key] = round(math.log(N/value), 2)\n",
    "\n",
    "for key, value in f_df.items():\n",
    "    f_idf[key] = round(math.log(N/value), 2)\n",
    "\n",
    "# m_idf = []\n",
    "# f_idf = []\n",
    "# for sentence in m_lem:\n",
    "#     idf = []\n",
    "#     for lemma in sentence:\n",
    "#         idf.append(round(math.log(N/m_df[lemma]), 2))\n",
    "#     m_idf.append(idf)\n",
    "\n",
    "# for sentence in f_lem:\n",
    "#     idf = []\n",
    "#     for lemma in sentence:\n",
    "#         idf.append(round(math.log(N/f_df[lemma]),2))\n",
    "#     f_idf.append(idf)\n",
    "\n",
    "m_idf = OrderedDict(sorted(m_idf.items(), key=lambda kv: kv[1], reverse=True))\n",
    "f_idf = OrderedDict(sorted(f_idf.items(), key=lambda kv: kv[1], reverse=True))\n",
    "\n",
    "print('idf m:', m_idf)\n",
    "print('idf f:', f_idf)\n",
    "\n",
    "#### COMPUTING TF-IDF ####\n",
    "m_tf_idfs = {}\n",
    "for i, sentence in enumerate(m_lem):\n",
    "    tf_idfs = []\n",
    "    for j, lemma in enumerate(sentence):\n",
    "        tf_idf = m_tf[i][j] * m_idf[lemma]\n",
    "        if lemma in m_tf_idfs:\n",
    "            m_tf_idfs[lemma] += tf_idf\n",
    "        else:\n",
    "            m_tf_idfs[lemma] = tf_idf\n",
    "\n",
    "for key, value in m_tf_idfs.items():\n",
    "    m_tf_idfs[key] = round(value / N, 2)\n",
    "\n",
    "f_tf_idfs = {}\n",
    "for i, sentence in enumerate(f_lem):\n",
    "    tf_idfs = []\n",
    "    for j, lemma in enumerate(sentence):\n",
    "        tf_idf = f_tf[i][j] * f_idf[lemma]\n",
    "        if lemma in f_tf_idfs:\n",
    "            f_tf_idfs[lemma] += tf_idf\n",
    "        else:\n",
    "            f_tf_idfs[lemma] = tf_idf\n",
    "\n",
    "for key, value in f_tf_idfs.items():\n",
    "    f_tf_idfs[key] = round(value / N, 2)\n",
    "\n",
    "\n",
    "m_tf_idfs = OrderedDict(sorted(m_tf_idfs.items(), key=lambda kv: kv[1], reverse=True))\n",
    "f_tf_idfs = OrderedDict(sorted(f_tf_idfs.items(), key=lambda kv: kv[1], reverse=True))\n",
    "\n",
    "print('------------------------------------------')\n",
    "print('sum tf-idf m:', m_tf_idfs)\n",
    "print('sum tf-idf f:', f_tf_idfs)\n",
    "\n",
    "\n",
    "print('------------------------------------------')\n",
    "import numpy as np\n",
    "\n",
    "def normalize_dict(d):\n",
    "    v_min = min(d.values())\n",
    "    v_max = max(d.values())\n",
    "    print(\"normalize v_min\", v_min, \"v_max\", v_max)\n",
    "    for k in d:\n",
    "        print(\"key\", k, \"d[k]-v_min\", d[k]-v_min, \"v_max-v_min\", v_max-v_min, \"result\", (d[k]-v_min)/(v_max-v_min))\n",
    "        d[k] = (d[k]-v_min)/(v_max-v_min)\n",
    "\n",
    "# test = np.array([0.3, 0.37, 0.3, 0.3, 0.3, 0.37, 0.37, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3])\n",
    "# norm = np.linalg.norm(test)\n",
    "# ok = test/norm\n",
    "# print(\"NORMMMMM \", ok)\n",
    "# normalize_dict(m_tf_idfs)\n",
    "# normalize_dict(f_tf_idfs)\n",
    "\n",
    "normalize_dict(m_tf_idfs)\n",
    "normalize_dict(f_tf_idfs)\n",
    "print(\"normalized m\", m_tf_idfs)\n",
    "\n",
    "\n",
    "print('----------------------------------------------------------')\n",
    "pol_dict = {key: f_tf_idfs[key] - m_tf_idfs.get(key, 0) for key in f_tf_idfs}\n",
    "for k, v in m_tf_idfs.items():\n",
    "    if k not in pol_dict:\n",
    "        pol_dict[k] = -v\n",
    "\n",
    "pol_dict = OrderedDict(sorted(pol_dict.items(), key=lambda kv: kv[1], reverse=True))\n",
    "print('polarity dict:', pol_dict)\n",
    "\n",
    "#idf\n",
    "# 'than', 'very', 'person', 'in', 'thrive', 'setting', 'competitive', 'strong', 'be', 'I', 'a'\n",
    "# 'than', 'pretty', 'John', 'support', 'very', 'person', 'in', 'thrive', 'set', 'nurture', 'be', 'I', 'a'\n",
    "\n",
    "#tf-idf\n",
    "# 'strong', 'be', 'I', 'a', 'than', 'very', 'person', 'thrive', 'in', 'competitive', 'setting',\n",
    "# 'pretty', 'be', 'I', 'a', 'than', 'John', 'very', 'support', 'person', 'thrive', 'in', 'nurture', 'set',\n",
    "\n",
    "#pol dict\n",
    "# 'pretty', 'than', 'John', 'very', 'support', 'person', 'thrive', 'in', 'nurture', 'set', 'competitive', 'setting', 'be', 'I', 'a', 'strong'\n",
    "# 'pretty', 1.0, 'than', 0.0, 'John', 0.0, 'very', 0.0, 'support', 0.0, 'person', 0.0, 'thrive', 0.0, 'in', 0.0, 'nurture', 0.0, 'set', 0.0, 'competitive', -0.0, 'setting', -0.0, 'be', -0.03425154041319317, 'I', -0.03425154041319317, 'a', -0.03425154041319317, 'strong', -1.0\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5ce0bbaaba556c243eee45087b7684ce43fda2aa9a5b0798832d1be21bf73af7"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
